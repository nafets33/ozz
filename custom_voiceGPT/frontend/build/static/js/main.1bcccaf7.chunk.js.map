{"version":3,"sources":["../node_modules/@vladmandic/face-api/dist sync","Dictaphone.jsx","VoiceGPT.jsx","Main.tsx","index.tsx"],"names":["webpackEmptyContext","req","e","Error","code","keys","resolve","module","exports","id","Dictaphone","_ref","commands","myFunc","listenAfterReply","noResponseTime","show_conversation","apiInProgress","transcribing","setTranscribing","useState","clearTranscriptOnListen","setClearTranscriptOnListen","finalTranscript","resetTranscript","listening","browserSupportsSpeechRecognition","isMicrophoneAvailable","useSpeechRecognition","prevScript","setPrevScript","useEffect","console","log","split","length","timer","setTimeout","i","keywords","api_body","j","keyword","RegExp","isKeywordFound","search","clearTimeout","React","createElement","Fragment","style","display","flexDirection","g_anwers","firstFace","CustomVoiceGPT","props","api","kwargs","height","width","show_video","input_text","no_response_time","face_recon","api_key","refresh_ask","imageSrc","setImageSrc","self_image","message","setMessage","answers","setAnswers","setListenAfterReply","modelsLoaded","setModelsLoaded","captureVideo","setCaptureVideo","textString","setTextString","setApiInProgress","faceData","useRef","faceTriggered","videoRef","canvasRef","audioRef","isListening","setIsListening","checkListeningStatus","SpeechRecognition","browserSupportsContinuousListening","startContinuousListening","async","ret","command","type","concat","text","user","body","tigger_type","face_data","current","data","axios","post","pause","stopListening","Audio","play","onended","listenContinuously","window","location","reload","error","startListening","continuous","language","Streamlit","setFrameHeight","intervalId","setInterval","clearInterval","Promise","all","faceapi","tinyFaceDetector","loadFromUri","process","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","ageGenderNet","then","loadModels","interval","className","toLowerCase","endsWith","controls","autoPlay","loop","muted","src","onClick","placeholder","value","onChange","target","onKeyDown","key","map","answer","idx","resp","textAlign","padding","closeWebcam","srcObject","getTracks","stop","cursor","backgroundColor","color","fontSize","border","borderRadius","startVideo","navigator","mediaDevices","getUserMedia","video","stream","catch","err","justifyContent","position","opacity","ref","onPlay","handleVideoOnPlay","innerHTML","displaySize","detections","withFaceLandmarks","withFaceExpressions","resizedDetections","hello_audio","getContext","clearRect","drawDetections","drawFaceLandmarks","drawFaceExpressions","withStreamlitConnection","args","VoiceGPT","engine","Styletron","ReactDOM","render","StrictMode","StyletronProvider","ThemeProvider","theme","LightTheme","Main","document","getElementById"],"mappings":"0HAAA,SAASA,EAAoBC,GAC5B,IAAIC,EAAI,IAAIC,MAAM,uBAAyBF,EAAM,KAEjD,MADAC,EAAEE,KAAO,mBACHF,EAEPF,EAAoBK,KAAO,WAAa,MAAO,IAC/CL,EAAoBM,QAAUN,EAC9BO,EAAOC,QAAUR,EACjBA,EAAoBS,GAAK,I,+IC2EVC,MAhFIC,IAOZ,IAPa,SAClBC,EAAQ,OACRC,EAAM,iBACNC,GAAmB,EAAK,eACxBC,EAAiB,EAAC,kBAClBC,GAAoB,EAAI,cACxBC,GAAgB,GACjBN,EACC,MAAOO,EAAcC,GAAmBC,oBAAS,IAC1CC,EAAyBC,GAA8BF,oBAAS,IACjE,gBAAEG,EAAe,gBAAEC,EAAe,UAAEC,EAAS,iCAAEC,EAAgC,sBAAEC,GAA0BC,+BAAqB,CAAEV,eAAcG,6BAC/IQ,EAAYC,GAAiBV,mBAAS,IAgD7C,OA9CAW,oBAAU,KACR,GAAwB,KAApBR,EAAwB,CAQ1B,GAPAS,QAAQC,IAAI,oBAAqBV,GAGjCS,QAAQC,IAAI,oBAAqBnB,GAI7BS,EAAgBW,MAAM,KAAKC,OAAS,GAGtC,OAFAH,QAAQC,IAAI,+CACZT,IAKFM,EAAcP,GAGd,MAAMa,EAAQC,WAAW,KACvB,IAAK,IAAIC,EAAI,EAAGA,EAAI1B,EAASuB,OAAQG,IAAK,CACxC,MAAM,SAAEC,EAAQ,SAAEC,GAAa5B,EAAS0B,GACxC,IAAK,IAAIG,EAAI,EAAGA,EAAIF,EAASJ,OAAQM,IAAK,CACxC,MAAMC,EAAU,IAAIC,OAAOJ,EAASE,GAAI,KAClCG,GAAsD,IAArCrB,EAAgBsB,OAAOH,GAE9C,GADAV,QAAQC,IAAI,oBAAqBnB,IAC5B8B,GAAkB9B,KAAsBG,EAO3C,OANIH,EACFD,EAAOU,EAAiB,CAAEiB,SAAU,CAAEE,QAAS,KAAQ,GAC9CE,GACT/B,EAAOU,EAAiBX,EAAS0B,GAAI,QAEvCd,KAMNQ,QAAQC,IAAI,gDACM,IAAjBlB,GAEH,MAAO,IAAM+B,aAAaV,KAE3B,CAACb,EAAiBT,EAAkBF,EAAUG,EAAgBS,EAAiBP,IAG7ES,EAIAC,EAKHoB,IAAAC,cAAAD,IAAAE,SAAA,KACGjC,GACC+B,IAAAC,cAAA,OAAKE,MAAO,CAAEC,QAAS,OAAQC,cAAe,WAC5CL,IAAAC,cAAA,YAAM,aAAWnB,GACjBkB,IAAAC,cAAA,YAAM,cAAYvB,EAAY,KAAO,OACrCsB,IAAAC,cAAA,YAAM,+BAA6B3B,EAA0B,KAAO,SATnE0B,IAAAC,cAAA,YAAM,yCAJND,IAAAC,cAAA,YAAM,uB,OCxDjB,IAEIK,EAAW,GACXC,GAAY,EAqYDC,MAnYSC,IACtB,MAAM,IAAEC,EAAG,OAAEC,EAAS,IAAOF,GACvB,SACJ5C,EAAQ,OACR+C,EAAM,MACNC,EAAK,kBACL5C,EAAiB,WACjB6C,EAAU,WACVC,EAAU,iBACVC,EAAgB,WAChBC,EAAU,QACVC,EAAO,YACPC,GACER,GACGS,EAAUC,GAAehD,mBAASsC,EAAOW,aACzCC,EAASC,GAAcnD,mBAAS,KAChCoD,EAASC,GAAcrD,mBAAS,KAChCN,EAAkB4D,GAAuBtD,oBAAS,IAClDuD,EAAcC,GAAmBxD,oBAAS,IAC1CyD,EAAcC,GAAmB1D,oBAAS,IAC1C2D,EAAYC,GAAiB5D,mBAAS,KACtCH,EAAegE,GAAoB7D,oBAAS,GAE7C8D,EAAWC,iBAAO,IAClBC,EAAgBD,kBAAO,GACvBE,EAAWF,mBAGXG,EAAYH,mBACZI,EAAWJ,iBAAO,OAEjBK,EAAaC,GAAkBrE,oBAAS,GAIzCsE,EAAuBA,KAEtBC,IAAkBC,sCAErBC,KAuGEhF,EAASiF,MAAOC,EAAKC,EAASC,KAClC1B,EAAW,KAAD2B,OAAMF,EAAkB,SAAW,QAAC,MAAAE,OAAKH,EAAG,MACtD,MAAMI,EAAO,IAAI9C,EAAU,CAAE+C,KAAML,IACnCtB,EAAW,IAAI0B,IACf,IACEnE,QAAQC,IAAI,wBAAyB+D,GACrCf,GAAiB,GACjB,MAAMoB,EAAO,CACXC,YAAaL,EACbhC,QAASA,EACTkC,KAAMA,EACN9B,WAAYF,EACZoC,UAAWrB,EAASsB,QACpBtC,YAAaA,GAEflC,QAAQC,IAAI,OACZ,MAAM,KAAEwE,SAAeC,IAAMC,KAAKlD,EAAK4C,GACvCrE,QAAQC,IAAI,YAAawE,EAAMJ,GAC/BI,EAAiB,YAAKrC,EAAYqC,EAAiB,YAC/ClB,EAASiB,SACXjB,EAASiB,QAAQI,QAGnBC,IAEAtB,EAASiB,QAAU,IAAIM,MAAML,EAAiB,YAC9ClB,EAASiB,QAAQO,OAEjBxB,EAASiB,QAAQQ,QAAU,KACzBhF,QAAQC,IAAI,4BAEZgF,IAEAvC,EAAoB+B,EAAyB,oBAE7CzE,QAAQC,IAAI,qBAAsBwE,EAAyB,oBAE3DhC,EAAWgC,EAAW,MACtBpD,EAAW,IAAIoD,EAAW,OAEE,IAAxBA,EAAkB,cACpBzE,QAAQC,IAAI,sBAAuBwE,EAAkB,aACrDS,OAAOC,SAASC,UAGlBnC,GAAiB,IAEnB,MAAOoC,GACPrF,QAAQC,IAAI,6BAA8BoF,GAC1CpC,GAAiB,KAIfY,EAA2BA,KAE/BF,IAAkB2B,eAAe,CAC/BC,YAAY,EACZC,SAAU,UAEZ/B,GAAe,IAGXoB,EAAgBA,KACpBlB,IAAkBkB,iBAMdI,EAAqBA,IACzBtB,IAAkB2B,eAAe,CAC/BC,YAAY,EACZC,SAAU,UAwCd,OA9BAzF,oBAAU,KACR0F,IAAUC,iBAGV,MAAMC,EAAaC,YAAYlC,EAAsB,KAErD,MAAO,KACLmC,cAAcF,KAEf,IAEH5F,oBAAU,KACW+D,WAGjBgC,QAAQC,IAAI,CACVC,IAAaC,iBAAiBC,YAHdC,YAIhBH,IAAaI,kBAAkBF,YAJfC,YAKhBH,IAAaK,mBAAmBH,YALhBC,YAMhBH,IAAaM,kBAAkBJ,YANfC,YAOhBH,IAAaO,aAAaL,YAPVC,cAQfK,KAAK,IAAM5D,GAAgB,KAEhC6D,GACA,MAAMC,EAAWd,YAAY,KAC3B5F,QAAQC,IAAI,wBAAyBiD,EAASsB,UAC7C,KACH,MAAO,IAAMqB,cAAca,IAC1B,IAGD3F,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAA,OAAK2F,UAAU,OACf5F,IAAAC,cAAA,WACKmB,GAAYA,EAASyE,cAAcC,SAAS,QAC3C9F,IAAAC,cAAA,SACEW,OAAQA,GAAU,IAClBC,MAAOA,GAAS,IAChBkF,UAAQ,EACRC,UAAU,EACVC,MAAM,EACNC,OAAK,GAELlG,IAAAC,cAAA,UAAQkG,IAAK/E,EAAU8B,KAAK,cAAc,gDAI5ClD,IAAAC,cAAA,OAAKkG,IAAK/E,EAAUR,OAAQA,GAAU,IAAKC,MAAOA,GAAS,OAG/Db,IAAAC,cAAA,OAAK2F,UAAU,OACb5F,IAAAC,cAACtC,EAAU,CACTE,SAAUA,EACVC,OAAQA,EACRC,iBAAkBA,EAClBC,eAAgBgD,EAChB/C,kBAAmBA,EACnBC,cAAeA,KAGnB8B,IAAAC,cAAA,OAAK2F,UAAU,cACb5F,IAAAC,cAAA,UAAQ2F,UAAU,kBAAkBQ,QAASlC,GAAoB,wBAIlEnD,GACCf,IAAAC,cAAA,OAAK2F,UAAU,cACb5F,IAAAC,cAAA,SACE2F,UAAU,eACV1C,KAAK,OACLmD,YAAY,kBACZC,MAAOtE,EACPuE,SA5PapJ,IACvB,MAAM,MAAEmJ,GAAUnJ,EAAEqJ,OACpBvE,EAAcqE,IA2PJG,UAxPatJ,IACT,UAAVA,EAAEuJ,MACJzH,QAAQC,IAAI,kBAAmB8C,GAC/BlE,EAAOkE,EAAY,CAAEvC,SAAU,CAAEE,QAAS,KAAQ,GAClDsC,EAAc,UAwPW,IAAtBhE,GACC+B,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAA,WAAK,SAAOsB,GACXE,EAAQkF,IAAI,CAACC,EAAQC,IACpB7G,IAAAC,cAAA,OAAKyG,IAAKG,GACR7G,IAAAC,cAAA,WAAK,UAAQ2G,EAAOvD,MACpBrD,IAAAC,cAAA,WAAK,UACK2G,EAAOE,KAAOF,EAAOE,KAAO,mBAOhD9G,IAAAC,cAAA,YAGAD,IAAAC,cAAA,WACGgB,GACCjB,IAAAC,cAAA,OAAKE,MAAO,CAAE4G,UAAW,SAAUC,QAAS,SACzClF,GAAgBF,EACf5B,IAAAC,cAAA,UACEmG,QA3LMa,KAClB3E,EAASmB,QAAQI,QACjBvB,EAASmB,QAAQyD,UAAUC,YAAY,GAAGC,OAC1CrF,GAAgB,IAyLJ5B,MAAO,CACLkH,OAAQ,UACRC,gBAAiB,QACjBC,MAAO,QACPP,QAAS,OACTQ,SAAU,OACVC,OAAQ,OACRC,aAAc,SAEjB,gBAID1H,IAAAC,cAAA,UACEmG,QAzRKuB,KACjB5F,GAAgB,GAChB6F,UAAUC,aACPC,aAAa,CAAEC,MAAO,CAAElH,MAAO,OAC/B4E,KAAMuC,IACL,IAAID,EAAQzF,EAASmB,QACrBsE,EAAMb,UAAYc,EAClBD,EAAM/D,SAEPiE,MAAOC,IACNjJ,QAAQqF,MAAM,SAAU4D,MAgRhB/H,MAAO,CACLkH,OAAQ,UACRC,gBAAiB,QACjBC,MAAO,QACPP,QAAS,OACTQ,SAAU,OACVC,OAAQ,OACRC,aAAc,SAEjB,gBAMN5F,EACCF,EACE5B,IAAAC,cAAA,WACED,IAAAC,cAAA,OACEE,MAAO,CACLC,QAAS,OACT+H,eAAgB,SAChBnB,QAAS,OACToB,SAAUtH,EAAa,GAAK,WAC5BuH,QAASvH,EAAa,EAAI,KAG5Bd,IAAAC,cAAA,SACEqI,IAAKhG,EACL1B,OAtVI,IAuVJC,MAtVG,IAuVH0H,OA3SUC,KACxB3D,YAAY9B,UACV,GAAIR,GAAaA,EAAUkB,QAAS,CAClClB,EAAUkB,QAAQgF,UAAYxD,IAC5B3C,EAASmB,SAEX,MAAMiF,EAAc,CAClB7H,MAnDW,IAoDXD,OArDY,KAwDdqE,IAAwB1C,EAAUkB,QAASiF,GAE3C,MAAMC,QAAmB1D,IAErB3C,EAASmB,QACT,IAAIwB,KAEL2D,oBACAC,sBAEGC,EAAoB7D,IAAsB0D,EAAYD,GAe5D,GAbII,EAAkB1J,OAAS,GAC7B+C,EAASsB,QAAUqF,GACdzG,EAAcoB,SAAWxC,IAC5BnD,EAAO,GAAI,CAAE2B,SAAU,CAAEE,QAAS,KAAQ,GAC1C0C,EAAcoB,SAAU,IAI1BnE,WAAW,KACT6C,EAASsB,QAAU,IAClB,KAGDqF,EAAkB1J,OAAS,IAAMmB,IACnCA,GAAY,EACRI,EAAOoI,aAAa,CACR,IAAIhF,MAAMpD,EAAOoI,aACzB/E,OAIVzB,GACEA,EAAUkB,SACVlB,EAAUkB,QACPuF,WAAW,MACXC,UAAU,EAAG,EA5FL,IADC,KA8Fd1G,GACEA,EAAUkB,SACVwB,IAAaiE,eAAe3G,EAAUkB,QAASqF,GACjDvG,GACEA,EAAUkB,SACVwB,IAAakE,kBAAkB5G,EAAUkB,QAASqF,GACpDvG,GACEA,EAAUkB,SACVwB,IAAamE,oBACX7G,EAAUkB,QACVqF,KAGL,MA8OW3I,MAAO,CAAEuH,aAAc,UAEzB1H,IAAAC,cAAA,UAAQqI,IAAK/F,EAAWpC,MAAO,CAAEiI,SAAU,gBAI/CpI,IAAAC,cAAA,WAAK,cAGPD,IAAAC,cAAAD,IAAAE,SAAA,SCtXKmJ,kBAVD5I,IACZ,MAAM,IAAEC,EAAG,OAAEC,GAAWF,EAAM6I,KAE9B,OADAtK,oBAAU,IAAM0F,IAAUC,kBAExB3E,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAACsJ,EAAQ,CAAC7I,IAAKA,EAAKC,OAAQA,O,gCCLlC,MAAM6I,EAAS,IAAIC,IAGnBC,IAASC,OACP3J,IAAAC,cAACD,IAAM4J,WAAU,KACf5J,IAAAC,cAAC4J,IAAiB,CAACvD,MAAOkD,GACxBxJ,IAAAC,cAAC6J,IAAa,CAACC,MAAOC,KACpBhK,IAAAC,cAACgK,EAAI,SAIXC,SAASC,eAAe,W","file":"static/js/main.1bcccaf7.chunk.js","sourcesContent":["function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = function() { return []; };\nwebpackEmptyContext.resolve = webpackEmptyContext;\nmodule.exports = webpackEmptyContext;\nwebpackEmptyContext.id = 20;","import React, { useState, useEffect } from \"react\";\nimport SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';\n\nconst Dictaphone = ({\n  commands,\n  myFunc,\n  listenAfterReply = false,\n  noResponseTime = 1,\n  show_conversation = true,\n  apiInProgress = false, // Receive apiInProgress as a prop\n}) => {\n  const [transcribing, setTranscribing] = useState(true);\n  const [clearTranscriptOnListen, setClearTranscriptOnListen] = useState(true);\n  const { finalTranscript, resetTranscript, listening, browserSupportsSpeechRecognition, isMicrophoneAvailable } = useSpeechRecognition({ transcribing, clearTranscriptOnListen });\n  const [prevScript, setPrevScript] = useState(\"\");\n\n  useEffect(() => {\n    if (finalTranscript !== \"\") {\n      console.log(\"Got final result:\", finalTranscript);\n\n      // Add logs to check the conditions\n      console.log(\"listenAfterReply:\", listenAfterReply);\n      // console.log(\"Number of words:\", finalTranscript.split(\" \").length);\n\n      // Clear the previous script if a keyword is found or if the transcript exceeds 89 words\n      if (finalTranscript.split(\" \").length > 89) {\n        console.log(\"Transcript exceeds 89 words. Clearing.\");\n        resetTranscript();\n        return;\n      }\n\n      // Set the previous script\n      setPrevScript(finalTranscript);\n\n      // Start the timer to check for keywords after a pause\n      const timer = setTimeout(() => {\n        for (let i = 0; i < commands.length; i++) {\n          const { keywords, api_body } = commands[i];\n          for (let j = 0; j < keywords.length; j++) {\n            const keyword = new RegExp(keywords[j], \"i\");\n            const isKeywordFound = finalTranscript.search(keyword) !== -1;\n            console.log(\"listenAfterReply:\", listenAfterReply);\n            if ((isKeywordFound || listenAfterReply) && !apiInProgress) {\n              if (listenAfterReply) {\n                myFunc(finalTranscript, { api_body: { keyword: \"\" } }, 3);\n              } else if (isKeywordFound) {\n                myFunc(finalTranscript, commands[i], 1);\n              }\n              resetTranscript();\n              return;\n            }\n          }\n        }\n        // Waiting for a keyword or API is in progress\n        console.log(\"Waiting for a keyword or API is in progress\");\n      }, noResponseTime * 1000);\n\n      return () => clearTimeout(timer); // Clear the timer on component unmount or when useEffect runs again\n    }\n  }, [finalTranscript, listenAfterReply, commands, noResponseTime, resetTranscript, apiInProgress]);\n\n\n  if (!browserSupportsSpeechRecognition) {\n    return <span>No browser support</span>;\n  }\n\n  if (!isMicrophoneAvailable) {\n    return <span>Please allow access to the microphone</span>;\n  }\n\n  return (\n    <>\n      {show_conversation && (\n        <div style={{ display: \"flex\", flexDirection: \"column\" }}>\n          <span>You said: {prevScript}</span>\n          <span>Listening: {listening ? \"on\" : \"off\"}</span>\n          <span>Clear Transcript On Listen: {clearTranscriptOnListen ? \"on\" : \"off\"}</span>\n        </div>\n      )}\n    </>\n  );\n};\n\nexport default Dictaphone;\n","import React, { useState, useEffect, useRef } from \"react\";\nimport axios from \"axios\";\nimport { Streamlit } from \"streamlit-component-lib\";\nimport SpeechRecognition from \"react-speech-recognition\";\nimport Dictaphone from \"./Dictaphone\";\nimport * as faceapi from \"@vladmandic/face-api\";\n\nlet timer = null;\nlet faceTimer = null;\nlet g_anwers = [];\nlet firstFace = false;\n\nconst CustomVoiceGPT = (props) => {\n  const { api, kwargs = {} } = props;\n  const {\n    commands,\n    height,\n    width,\n    show_conversation,\n    show_video,\n    input_text,\n    no_response_time,\n    face_recon,\n    api_key,\n    refresh_ask,\n  } = kwargs;\n  const [imageSrc, setImageSrc] = useState(kwargs.self_image);\n  const [message, setMessage] = useState(\"\");\n  const [answers, setAnswers] = useState([]);\n  const [listenAfterReply, setListenAfterReply] = useState(false);\n  const [modelsLoaded, setModelsLoaded] = useState(false);\n  const [captureVideo, setCaptureVideo] = useState(false);\n  const [textString, setTextString] = useState(\"\");\n  const [apiInProgress, setApiInProgress] = useState(false); // Added state for API in progress\n\n  const faceData = useRef([]);\n  const faceTriggered = useRef(false);\n  const videoRef = useRef();\n  const videoHeight = 480;\n  const videoWidth = 640;\n  const canvasRef = useRef();\n  const audioRef = useRef(null);\n\n  const [isListening, setIsListening] = useState(true);\n\n  // ... (other code)\n\n  const checkListeningStatus = () => {\n    // Check if continuous listening is active\n    if (!SpeechRecognition.browserSupportsContinuousListening()) {\n      // If not, restart continuous listening\n      startContinuousListening();\n    }\n  };\n\n\n  const handleInputText = (e) => {\n    const { value } = e.target;\n    setTextString(value);\n  };\n\n  const handleOnKeyDown = (e) => {\n    if (e.key === \"Enter\") {\n      console.log(\"textString :>> \", textString);\n      myFunc(textString, { api_body: { keyword: \"\" } }, 4);\n      setTextString(\"\");\n    }\n  };\n\n  const startVideo = () => {\n    setCaptureVideo(true);\n    navigator.mediaDevices\n      .getUserMedia({ video: { width: 300 } })\n      .then((stream) => {\n        let video = videoRef.current;\n        video.srcObject = stream;\n        video.play();\n      })\n      .catch((err) => {\n        console.error(\"error:\", err);\n      });\n  };\n\n  const handleVideoOnPlay = () => {\n    setInterval(async () => {\n      if (canvasRef && canvasRef.current) {\n        canvasRef.current.innerHTML = faceapi.createCanvasFromMedia(\n          videoRef.current\n        );\n        const displaySize = {\n          width: videoWidth,\n          height: videoHeight,\n        };\n\n        faceapi.matchDimensions(canvasRef.current, displaySize);\n\n        const detections = await faceapi\n          .detectAllFaces(\n            videoRef.current,\n            new faceapi.TinyFaceDetectorOptions()\n          )\n          .withFaceLandmarks()\n          .withFaceExpressions();\n\n        const resizedDetections = faceapi.resizeResults(detections, displaySize);\n\n        if (resizedDetections.length > 0) {\n          faceData.current = resizedDetections;\n          if (!faceTriggered.current && face_recon) {\n            myFunc(\"\", { api_body: { keyword: \"\" } }, 2);\n            faceTriggered.current = true;\n          }\n        } else {\n          faceTimer && clearTimeout(faceTimer);\n          setTimeout(() => {\n            faceData.current = [];\n          }, 1000);\n        }\n\n        if (resizedDetections.length > 0 && !firstFace) {\n          firstFace = true;\n          if (kwargs.hello_audio) {\n            const audio = new Audio(kwargs.hello_audio);\n            audio.play();\n          }\n        }\n\n        canvasRef &&\n          canvasRef.current &&\n          canvasRef.current\n            .getContext(\"2d\")\n            .clearRect(0, 0, videoWidth, videoHeight);\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawDetections(canvasRef.current, resizedDetections);\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawFaceLandmarks(canvasRef.current, resizedDetections);\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawFaceExpressions(\n            canvasRef.current,\n            resizedDetections\n          );\n      }\n    }, 300);\n  };\n\n  const closeWebcam = () => {\n    videoRef.current.pause();\n    videoRef.current.srcObject.getTracks()[0].stop();\n    setCaptureVideo(false);\n  };\n\n  const myFunc = async (ret, command, type) => {\n    setMessage(` (${command[\"api_body\"][\"keyword\"]}) ${ret},`);\n    const text = [...g_anwers, { user: ret }];\n    setAnswers([...text]);\n    try {\n      console.log(\"api call on listen...\", command);\n      setApiInProgress(true); // Set API in progress to true\n      const body = {\n        tigger_type: type,\n        api_key: api_key,\n        text: text,\n        self_image: imageSrc,\n        face_data: faceData.current,\n        refresh_ask: refresh_ask,\n      };\n      console.log(\"api\");\n      const { data } = await axios.post(api, body);\n      console.log(\"data :>> \", data, body);\n      data[\"self_image\"] && setImageSrc(data[\"self_image\"]);\n      if (audioRef.current) {\n        audioRef.current.pause(); // Pause existing playback if any\n      }\n\n      stopListening(); // turn off listen\n\n      audioRef.current = new Audio(data[\"audio_path\"]);\n      audioRef.current.play();\n\n      audioRef.current.onended = () => {\n        console.log(\"Audio playback finished.\");\n\n        listenContinuously();\n\n        setListenAfterReply(data[\"listen_after_reply\"]);\n\n        console.log(\"listen after reply\", data[\"listen_after_reply\"]);\n\n        setAnswers(data[\"text\"]);\n        g_anwers = [...data[\"text\"]];\n\n        if (data[\"page_direct\"] === true) {\n          console.log(\"api has page direct\", data[\"page_direct\"]);\n          window.location.reload();\n        }\n\n        setApiInProgress(false); // Set API in progress to false after completion\n      };\n    } catch (error) {\n      console.log(\"api call on listen failed!\", error);\n      setApiInProgress(false); // Set API in progress to false on error\n    }\n  };\n\n  const startContinuousListening = () => {\n    // Start continuous listening\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"en-GB\",\n    });\n    setIsListening(true);\n  };\n\n  const stopListening = () => {\n    SpeechRecognition.stopListening();\n  };\n  const startListening = () => {\n    SpeechRecognition.startListening();\n  };\n\n  const listenContinuously = () =>\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"en-GB\",\n    });\n  const listenContinuouslyInChinese = () =>\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"zh-CN\",\n    });\n  const listenOnce = () =>\n    SpeechRecognition.startListening({ continuous: false });\n\n  useEffect(() => {\n    Streamlit.setFrameHeight();\n\n    // Check listening status every minute\n    const intervalId = setInterval(checkListeningStatus, 60000);\n\n    return () => {\n      clearInterval(intervalId);\n    };\n  }, []);\n  \n  useEffect(() => {\n    const loadModels = async () => {\n      const MODEL_URL = process.env.PUBLIC_URL + \"/models\";\n\n      Promise.all([\n        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),\n        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),\n        faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),\n      ]).then(() => setModelsLoaded(true));\n    };\n    loadModels();\n    const interval = setInterval(() => {\n      console.log(\"faceData.current :>> \", faceData.current);\n    }, 3000);\n    return () => clearInterval(interval);\n  }, []);\n\n  return (\n    <>\n      <div className=\"p-2\">\n      <div>\n          {imageSrc && imageSrc.toLowerCase().endsWith(\".mp4\") ? (\n            <video\n              height={height || 100}\n              width={width || 100}\n              controls\n              autoPlay={true} // Use a variable to control autoplay shouldAutoplay\n              loop={false}\n              muted\n            >\n              <source src={imageSrc} type=\"video/mp4\" />\n              Your browser does not support the video tag.\n            </video>\n          ) : (\n            <img src={imageSrc} height={height || 100} width={width || 100} />\n          )}\n        </div>\n        <div className=\"p-2\">\n          <Dictaphone\n            commands={commands}\n            myFunc={myFunc}\n            listenAfterReply={listenAfterReply}\n            noResponseTime={no_response_time}\n            show_conversation={show_conversation}\n            apiInProgress={apiInProgress} // Pass down API in progress\n          />\n        </div>\n        <div className=\"form-group\">\n          <button className=\"btn btn-primary\" onClick={listenContinuously}>\n            Listen continuously\n          </button>\n        </div>\n        {input_text && (\n          <div className=\"form-group\">\n            <input\n              className=\"form-control\"\n              type=\"text\"\n              placeholder=\"Chat with Hoots\"\n              value={textString}\n              onChange={handleInputText}\n              onKeyDown={handleOnKeyDown}\n            />\n          </div>\n        )}\n        {show_conversation === true && (\n          <>\n            <div> You: {message}</div>\n            {answers.map((answer, idx) => (\n              <div key={idx}>\n                <div>-user: {answer.user}</div>\n                <div>\n                  -resp: {answer.resp ? answer.resp : \"thinking...\"}\n                </div>\n              </div>\n            ))}\n          </>\n        )}\n      </div>\n      <div>\n        {/* ... (rest of your code) */}\n      </div>\n      <div>\n        {face_recon && (\n          <div style={{ textAlign: \"center\", padding: \"10px\" }}>\n            {captureVideo && modelsLoaded ? (\n              <button\n                onClick={closeWebcam}\n                style={{\n                  cursor: \"pointer\",\n                  backgroundColor: \"green\",\n                  color: \"white\",\n                  padding: \"15px\",\n                  fontSize: \"25px\",\n                  border: \"none\",\n                  borderRadius: \"10px\",\n                }}\n              >\n                Close Webcam\n              </button>\n            ) : (\n              <button\n                onClick={startVideo}\n                style={{\n                  cursor: \"pointer\",\n                  backgroundColor: \"green\",\n                  color: \"white\",\n                  padding: \"15px\",\n                  fontSize: \"25px\",\n                  border: \"none\",\n                  borderRadius: \"10px\",\n                }}\n              >\n                Open Webcam\n              </button>\n            )}\n          </div>\n        )}\n        {captureVideo ? (\n          modelsLoaded ? (\n            <div>\n              <div\n                style={{\n                  display: \"flex\",\n                  justifyContent: \"center\",\n                  padding: \"10px\",\n                  position: show_video ? \"\" : \"absolute\",\n                  opacity: show_video ? 1 : 0.3,\n                }}\n              >\n                <video\n                  ref={videoRef}\n                  height={videoHeight}\n                  width={videoWidth}\n                  onPlay={handleVideoOnPlay}\n                  style={{ borderRadius: \"10px\" }}\n                />\n                <canvas ref={canvasRef} style={{ position: \"absolute\" }} />\n              </div>\n            </div>\n          ) : (\n            <div>loading...</div>\n          )\n        ) : (\n          <></>\n        )}\n      </div>\n    </>\n  );\n};\n\nexport default CustomVoiceGPT;\n","import React, { useEffect, useState } from \"react\"\nimport {\n  ComponentProps,\n  Streamlit,\n  withStreamlitConnection,\n} from \"streamlit-component-lib\"\nimport VoiceGPT from \"./VoiceGPT.jsx\"\n\nconst Main = (props: ComponentProps) => {\n  const { api, kwargs } = props.args\n  useEffect(() => Streamlit.setFrameHeight())\n  return (\n    <>\n      <VoiceGPT api={api} kwargs={kwargs} />\n    </>\n  )\n}\n\nexport default withStreamlitConnection(Main)\n","import React from \"react\"\nimport ReactDOM from \"react-dom\"\nimport Main from \"./Main\"\n// Lots of import to define a Styletron engine and load the light theme of baseui\nimport { Client as Styletron } from \"styletron-engine-atomic\"\nimport { Provider as StyletronProvider } from \"styletron-react\"\nimport { ThemeProvider, LightTheme } from \"baseui\"\n\nconst engine = new Styletron()\n\n// Wrap your CustomSlider with the baseui them\nReactDOM.render(\n  <React.StrictMode>\n    <StyletronProvider value={engine}>\n      <ThemeProvider theme={LightTheme}>\n        <Main />\n      </ThemeProvider>\n    </StyletronProvider>\n  </React.StrictMode>,\n  document.getElementById(\"root\")\n)\n"],"sourceRoot":""}