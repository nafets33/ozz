{"version":3,"sources":["../node_modules/@vladmandic/face-api/dist sync","Dictaphone.jsx","VoiceGPT.jsx","Main.tsx","index.tsx"],"names":["webpackEmptyContext","req","e","Error","code","keys","resolve","module","exports","id","timer","Dictaphone","_ref","commands","myFunc","listenAfterRelpy","noResponseTime","show_conversation","transcribing","setTranscribing","useState","clearTranscriptOnListen","setClearTranscriptOnListen","transcript","interimTranscript","finalTranscript","resetTranscript","listening","browserSupportsSpeechRecognition","isMicrophoneAvailable","useSpeechRecognition","prevScript","setPrevScript","useEffect","console","log","clearTimeout","setTimeout","i","length","keywords","api_body","j","keyword","RegExp","search","React","createElement","Fragment","style","display","flexDirection","g_anwers","firstFace","CustomVoiceGPT","props","api","kwargs","height","width","show_video","input_text","no_response_time","face_recon","imageSrc","setImageSrc","self_image","message","setMessage","answers","setAnswers","setListenAfterReply","modelsLoaded","setModelsLoaded","captureVideo","setCaptureVideo","textString","setTextString","faceData","useRef","faceTriggered","videoRef","canvasRef","audioRef","async","ret","command","type","concat","text","user","body","tigger_type","api_key","face_data","current","data","axios","post","pause","Audio","play","onended","window","location","reload","error","Streamlit","setFrameHeight","Promise","all","faceapi","tinyFaceDetector","loadFromUri","process","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","then","loadModels","interval","setInterval","clearInterval","className","src","onClick","listenContinuously","SpeechRecognition","startListening","continuous","language","placeholder","value","onChange","target","onKeyDown","key","map","answer","idx","resp","textAlign","padding","closeWebcam","srcObject","getTracks","stop","cursor","backgroundColor","color","fontSize","border","borderRadius","startVideo","navigator","mediaDevices","getUserMedia","video","stream","catch","err","justifyContent","position","opacity","ref","onPlay","handleVideoOnPlay","innerHTML","displaySize","detections","withFaceLandmarks","withFaceExpressions","resizedDetections","hello_audio","getContext","clearRect","drawDetections","drawFaceLandmarks","drawFaceExpressions","withStreamlitConnection","args","VoiceGPT","engine","Styletron","ReactDOM","render","StrictMode","StyletronProvider","ThemeProvider","theme","LightTheme","Main","document","getElementById"],"mappings":"0HAAA,SAASA,EAAoBC,GAC5B,IAAIC,EAAI,IAAIC,MAAM,uBAAyBF,EAAM,KAEjD,MADAC,EAAEE,KAAO,mBACHF,EAEPF,EAAoBK,KAAO,WAAa,MAAO,IAC/CL,EAAoBM,QAAUN,EAC9BO,EAAOC,QAAUR,EACjBA,EAAoBS,GAAK,I,+ICLzB,IAAIC,EAgGWC,MA9FIC,IAMZ,IANa,SAClBC,EAAQ,OACRC,EAAM,iBACNC,EAAgB,eAChBC,EAAiB,EAAC,kBAClBC,GAAoB,GACrBL,EACC,MAAOM,EAAcC,GAAmBC,oBAAS,IAC1CC,EAAyBC,GAA8BF,oBAAS,IAIjE,WACJG,EAAU,kBACVC,EAAiB,gBACjBC,EAAe,gBACfC,EAAe,UACfC,EAAS,iCACTC,EAAgC,sBAChCC,GACEC,+BAAqB,CAAEZ,eAAcG,6BAClCU,EAAYC,GAAiBZ,mBAAS,IAkD7C,OAhDAa,oBAAU,OAUP,CAACT,IAEJS,oBAAU,KACe,IAAnBR,IACFS,QAAQC,IAAI,oBAAqBV,GACjCf,GAAS0B,aAAa1B,GACtBA,EAAQ2B,WAAW,KACjBL,EAAcP,GAEd,IAAK,IAAIa,EAAI,EAAGA,EAAIzB,EAAS0B,OAAQD,IAAK,CACxC,MAAM,SAAEE,EAAQ,SAAEC,GAAa5B,EAASyB,GACxC,IAAK,IAAII,EAAI,EAAGA,EAAIF,EAASD,OAAQG,IAAK,CACxC,MAAMC,EAAU,IAAIC,OAAOJ,EAASE,GAAI,KAGxC,IAF2D,GAApCjB,EAAgBoB,OAAOF,GAK5C,OAFA7B,EAAOW,EAAiBZ,EAASyB,GAAI,QACrCZ,KAKN,GAAIX,EAGF,OAFAD,EAAOW,EAAiB,CAAEgB,SAAU,CAAEE,QAAS,KAAQ,QACvDjB,IAIFQ,QAAQC,IAAI,uBACZT,KACkB,IAAjBV,IAEkB,IAAnBS,GAA0BV,IAC5BiB,EAAcP,GACdC,MAED,CAACD,EAAiBV,EAAkBF,IAElCe,EAIAC,EAKHiB,IAAAC,cAAAD,IAAAE,SAAA,KACG/B,GACC6B,IAAAC,cAAA,OAAKE,MAAO,CAAEC,QAAS,OAAQC,cAAe,WAC5CL,IAAAC,cAAA,YAAM,aAAWhB,GACjBe,IAAAC,cAAA,YAAM,cAAYpB,EAAY,KAAO,OACrCmB,IAAAC,cAAA,YAAM,4BACsB1B,EAA0B,KAAO,SAV5DyB,IAAAC,cAAA,YAAM,yCAJND,IAAAC,cAAA,YAAM,uB,OC3DjB,IAEIK,EAAW,GACXC,GAAY,EAiZDC,MA/YSC,IACtB,MAAM,IAAEC,EAAG,OAAEC,EAAS,IAAOF,GACvB,SACJ1C,EAAQ,OACR6C,EAAM,MACNC,EAAK,kBACL1C,EAAiB,WACjB2C,EAAU,WACVC,EAAU,iBACVC,EAAgB,WAChBC,GACEN,GACGO,EAAUC,GAAe7C,mBAASqC,EAAOS,aACzCC,EAASC,GAAchD,mBAAS,KAChCiD,EAASC,GAAclD,mBAAS,KAChCL,EAAkBwD,GAAuBnD,oBAAS,IAClDoD,EAAcC,GAAmBrD,oBAAS,IAC1CsD,EAAcC,GAAmBvD,oBAAS,IAC1CwD,EAAYC,GAAiBzD,mBAAS,IAEvC0D,EAAWC,iBAAO,IAClBC,EAAgBD,kBAAO,GACvBE,EAAWF,mBAGXG,EAAYH,mBACZI,EAAWJ,iBAAO,MA8GlBjE,EAASsE,MAAOC,EAAKC,EAASC,KAClCnB,EAAW,KAADoB,OAAMF,EAAkB,SAAW,QAAC,MAAAE,OAAKH,EAAG,MACtD,MAAMI,EAAO,IAAIrC,EAAU,CAAEsC,KAAML,IACnCf,EAAW,IAAImB,IACf,IACEvD,QAAQC,IAAI,wBAAyBmD,GACrC,MAAMK,EAAO,CACXC,YAAaL,EACbM,QAAS,UACTJ,KAAMA,EACNvB,WAAYF,EACZ8B,UAAWhB,EAASiB,SAEtB7D,QAAQC,IAAI,OACZ,MAAM,KAAE6D,SAAeC,IAAMC,KAAK1C,EAAKmC,GACvCzD,QAAQC,IAAI,YAAa6D,EAAML,GAC/BK,EAAiB,YAAK/B,EAAY+B,EAAiB,YAC/CA,EAAiB,YACfb,EAASY,SACXZ,EAASY,QAAQI,QAEnBhB,EAASY,QAAU,IAAIK,MAAMJ,EAAiB,YAC9Cb,EAASY,QAAQM,OAKjBlB,EAASY,QAAQO,QAAU,KACzBpE,QAAQC,IAAI,4BAER6D,EAAyB,oBAC3BzB,EAAoByB,EAAyB,oBAG/C1B,EAAW0B,EAAW,MACtB5C,EAAW,IAAI4C,EAAW,OAEE,IAAxBA,EAAkB,cACpB9D,QAAQC,IAAI,sBAAuB6D,EAAkB,aACrDO,OAAOC,SAASC,aAIhBT,EAAyB,oBAC3BzB,EAAoByB,EAAyB,oBAG/C1B,EAAW0B,EAAW,MACtB5C,EAAW,IAAI4C,EAAW,OAEE,IAAxBA,EAAkB,cACpB9D,QAAQC,IAAI,sBAAuB6D,EAAkB,aACrDO,OAAOC,SAASC,WAGpB,MAAOC,GACPxE,QAAQC,IAAI,8BAA+BuE,KAwF/C,OAtBAzE,oBAAU,IAAM0E,IAAUC,kBAE1B3E,oBAAU,OAAU,CAACsB,IAErBtB,oBAAU,KACWmD,WAGjByB,QAAQC,IAAI,CACVC,IAAaC,iBAAiBC,YAHdC,YAIhBH,IAAaI,kBAAkBF,YAJfC,YAKhBH,IAAaK,mBAAmBH,YALhBC,YAMhBH,IAAaM,kBAAkBJ,YANfC,cAOfI,KAAK7C,GAAgB,KAE1B8C,GACA,MAAMC,EAAWC,YAAY,KAC3BvF,QAAQC,IAAI,wBAAyB2C,EAASiB,UAC7C,KACH,MAAO,IAAM2B,cAAcF,IAC1B,IAGD1E,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAA,OAAK4E,UAAU,OACb7E,IAAAC,cAAA,WACED,IAAAC,cAAA,OAAK6E,IAAK5D,EAAUN,OAAQA,GAAU,IAAKC,MAAOA,GAAS,OAE7Db,IAAAC,cAAA,OAAK4E,UAAU,OACb7E,IAAAC,cAACpC,EAAU,CACTE,SAAUA,EACVC,OAAQA,EACRC,iBAAkBA,EAClBC,eAAgB8C,EAChB7C,kBAAmBA,KAGvB6B,IAAAC,cAAA,OAAK4E,UAAU,cACb7E,IAAAC,cAAA,UAAQ4E,UAAU,kBAAkBE,QAnDjBC,IACzBC,IAAkBC,eAAe,CAC/BC,YAAY,EACZC,SAAU,WAgD2D,wBAIlErE,GACCf,IAAAC,cAAA,OAAK4E,UAAU,cACb7E,IAAAC,cAAA,SACE4E,UAAU,eACVpC,KAAK,OACL4C,YAAY,oBACZC,MAAOxD,EACPyD,SAvRanI,IACvB,MAAM,MAAEkI,GAAUlI,EAAEoI,OACpBzD,EAAcuD,IAsRJG,UAnRarI,IACT,UAAVA,EAAEsI,MACJtG,QAAQC,IAAI,kBAAmByC,GAC/B9D,EAAO8D,EAAY,CAAEnC,SAAU,CAAEE,QAAS,KAAQ,GAClDkC,EAAc,UAmRW,IAAtB5D,GACC6B,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAA,WAAK,SAAOoB,GACXE,EAAQoE,IAAI,CAACC,EAAQC,IACpB7F,IAAAC,cAAA,OAAKyF,IAAKG,GACR7F,IAAAC,cAAA,WAAK,UAAQ2F,EAAOhD,MACpB5C,IAAAC,cAAA,WAAK,UAAQ2F,EAAOE,KAAOF,EAAOE,KAAO,mBAMnD9F,IAAAC,cAAA,YAMAD,IAAAC,cAAA,WACGgB,GACCjB,IAAAC,cAAA,OAAKE,MAAO,CAAE4F,UAAW,SAAUC,QAAS,SACzCpE,GAAgBF,EACf1B,IAAAC,cAAA,UACE8E,QA1NMkB,KAClB9D,EAASc,QAAQI,QACjBlB,EAASc,QAAQiD,UAAUC,YAAY,GAAGC,OAC1CvE,GAAgB,IAwNJ1B,MAAO,CACLkG,OAAQ,UACRC,gBAAiB,QACjBC,MAAO,QACPP,QAAS,OACTQ,SAAU,OACVC,OAAQ,OACRC,aAAc,SAEjB,gBAID1G,IAAAC,cAAA,UACE8E,QArTK4B,KACjB9E,GAAgB,GAChB+E,UAAUC,aACPC,aAAa,CAAEC,MAAO,CAAElG,MAAO,OAC/B2D,KAAMwC,IACL,IAAID,EAAQ5E,EAASc,QACrB8D,EAAMb,UAAYc,EAClBD,EAAMxD,SAEP0D,MAAOC,IACN9H,QAAQwE,MAAM,SAAUsD,MA4ShB/G,MAAO,CACLkG,OAAQ,UACRC,gBAAiB,QACjBC,MAAO,QACPP,QAAS,OACTQ,SAAU,OACVC,OAAQ,OACRC,aAAc,SAEjB,gBAMN9E,EACCF,EACE1B,IAAAC,cAAA,WACED,IAAAC,cAAA,OACEE,MAAO,CACLC,QAAS,OACT+G,eAAgB,SAChBnB,QAAS,OACToB,SAAUtG,EAAa,GAAK,WAC5BuG,QAASvG,EAAa,EAAI,KAG5Bd,IAAAC,cAAA,SACEqH,IAAKnF,EACLvB,OArWI,IAsWJC,MArWG,IAsWH0G,OAvUUC,KACxB7C,YAAYrC,UACV,GAAIF,GAAaA,EAAUa,QAAS,CAClCb,EAAUa,QAAQwE,UAAYxD,IAC5B9B,EAASc,SAEX,MAAMyE,EAAc,CAClB7G,MAtCW,IAuCXD,OAxCY,KA2CdqD,IAAwB7B,EAAUa,QAASyE,GAE3C,MAAMC,QAAmB1D,IAErB9B,EAASc,QACT,IAAIgB,KAEL2D,oBACAC,sBAEGC,EAAoB7D,IAAsB0D,EAAYD,GAe5D,GAbII,EAAkBrI,OAAS,GAC7BuC,EAASiB,QAAU6E,GACd5F,EAAce,SAAWhC,IAC5BjD,EAAO,GAAI,CAAE2B,SAAU,CAAEE,QAAS,KAAQ,GAC1CqC,EAAce,SAAU,IAI1B1D,WAAW,KACTyC,EAASiB,QAAU,IAClB,KAGD6E,EAAkBrI,OAAS,IAAMc,IACnCA,GAAY,EACRI,EAAOoH,aAAa,CACR,IAAIzE,MAAM3C,EAAOoH,aACzBxE,OAIVnB,GACEA,EAAUa,SACVb,EAAUa,QACP+E,WAAW,MACXC,UAAU,EAAG,EA/EL,IADC,KAiFd7F,GACEA,EAAUa,SACVgB,IAAaiE,eAAe9F,EAAUa,QAAS6E,GACjD1F,GACEA,EAAUa,SACVgB,IAAakE,kBAAkB/F,EAAUa,QAAS6E,GACpD1F,GACEA,EAAUa,SACVgB,IAAamE,oBAAoBhG,EAAUa,QAAS6E,KAEvD,MA6QW3H,MAAO,CAAEuG,aAAc,UAEzB1G,IAAAC,cAAA,UAAQqH,IAAKlF,EAAWjC,MAAO,CAAEiH,SAAU,gBAI/CpH,IAAAC,cAAA,WAAK,cAGPD,IAAAC,cAAAD,IAAAE,SAAA,SC7YKmI,kBAVD5H,IACZ,MAAM,IAAEC,EAAG,OAAEC,GAAWF,EAAM6H,KAE9B,OADAnJ,oBAAU,IAAM0E,IAAUC,kBAExB9D,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAACsI,EAAQ,CAAC7H,IAAKA,EAAKC,OAAQA,O,gCCLlC,MAAM6H,EAAS,IAAIC,IAGnBC,IAASC,OACP3I,IAAAC,cAACD,IAAM4I,WAAU,KACf5I,IAAAC,cAAC4I,IAAiB,CAACvD,MAAOkD,GACxBxI,IAAAC,cAAC6I,IAAa,CAACC,MAAOC,KACpBhJ,IAAAC,cAACgJ,EAAI,SAIXC,SAASC,eAAe,W","file":"static/js/main.1c080d60.chunk.js","sourcesContent":["function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = function() { return []; };\nwebpackEmptyContext.resolve = webpackEmptyContext;\nmodule.exports = webpackEmptyContext;\nwebpackEmptyContext.id = 20;","import React, { useState, useEffect } from \"react\"\nimport { useSpeechRecognition } from \"react-speech-recognition\"\n\nlet timer\n\nconst Dictaphone = ({\n  commands,\n  myFunc,\n  listenAfterRelpy,\n  noResponseTime = 1,\n  show_conversation = true,\n}) => {\n  const [transcribing, setTranscribing] = useState(true)\n  const [clearTranscriptOnListen, setClearTranscriptOnListen] = useState(true)\n  const toggleTranscribing = () => setTranscribing(!transcribing)\n  const toggleClearTranscriptOnListen = () =>\n    setClearTranscriptOnListen(!clearTranscriptOnListen)\n  const {\n    transcript,\n    interimTranscript,\n    finalTranscript,\n    resetTranscript,\n    listening,\n    browserSupportsSpeechRecognition,\n    isMicrophoneAvailable,\n  } = useSpeechRecognition({ transcribing, clearTranscriptOnListen })\n  const [prevScript, setPrevScript] = useState(\"\")\n\n  useEffect(() => {\n    // console.log(\n    //   \"Got interim result:\",\n    //   interimTranscript.length,\n    //   interimTranscript\n    // )\n    // setPrevScript(interimTranscript)\n    // if (interimTranscript === \"\") {\n    //   console.log(\"prevScript :>> \", prevScript)\n    // }\n  }, [interimTranscript])\n\n  useEffect(() => {\n    if (finalTranscript != \"\") {\n      console.log(\"Got final result:\", finalTranscript)\n      timer && clearTimeout(timer)\n      timer = setTimeout(() => {\n        setPrevScript(finalTranscript)\n        // keyword trigger\n        for (let i = 0; i < commands.length; i++) {\n          const { keywords, api_body } = commands[i]\n          for (let j = 0; j < keywords.length; j++) {\n            const keyword = new RegExp(keywords[j], \"i\")\n            const isKeywordFound = finalTranscript.search(keyword) != -1\n\n            if (isKeywordFound) {\n              myFunc(finalTranscript, commands[i], 1)\n              resetTranscript()\n              return\n            }\n          }\n        }\n        if (listenAfterRelpy) {\n          myFunc(finalTranscript, { api_body: { keyword: \"\" } }, 3)\n          resetTranscript()\n          return\n        }\n        //waiting for keyword\n        console.log(\"waiting for keyword\")\n        resetTranscript()\n      }, noResponseTime * 1000)\n    }\n    if (finalTranscript != \"\" && !listenAfterRelpy) {\n      setPrevScript(finalTranscript)\n      resetTranscript()\n    }\n  }, [finalTranscript, listenAfterRelpy, commands])\n\n  if (!browserSupportsSpeechRecognition) {\n    return <span>No browser support</span>\n  }\n\n  if (!isMicrophoneAvailable) {\n    return <span>Please allow access to the microphone</span>\n  }\n\n  return (\n    <>\n      {show_conversation && (\n        <div style={{ display: \"flex\", flexDirection: \"column\" }}>\n          <span>you said: {prevScript}</span>\n          <span>listening: {listening ? \"on\" : \"off\"}</span>\n          <span>\n            clearTranscriptOnListen: {clearTranscriptOnListen ? \"on\" : \"off\"}\n          </span>\n        </div>\n      )}\n    </>\n  )\n}\n\nexport default Dictaphone\n","import React, { useState, useEffect, FC, memo, useMemo, useRef } from \"react\"\nimport axios from \"axios\"\nimport {\n  ComponentProps,\n  Streamlit,\n  withStreamlitConnection,\n} from \"streamlit-component-lib\"\nimport SpeechRecognition, {\n  useSpeechRecognition,\n} from \"react-speech-recognition\"\nimport Dictaphone from \"./Dictaphone\"\nimport * as faceapi from \"@vladmandic/face-api\"\n\nconst imageUrls = {\n  hoots: \"/hoots.png\",\n  hootsAndHootie: \"/hootsAndhootie.png\",\n}\n\nlet timer = null\nlet faceTimer = null\nlet g_anwers = []\nlet firstFace = false\n\nconst CustomVoiceGPT = (props) => {\n  const { api, kwargs = {} } = props\n  const {\n    commands,\n    height,\n    width,\n    show_conversation,\n    show_video,\n    input_text,\n    no_response_time,\n    face_recon,\n  } = kwargs\n  const [imageSrc, setImageSrc] = useState(kwargs.self_image)\n  const [message, setMessage] = useState(\"\")\n  const [answers, setAnswers] = useState([])\n  const [listenAfterRelpy, setListenAfterReply] = useState(false)\n  const [modelsLoaded, setModelsLoaded] = useState(false)\n  const [captureVideo, setCaptureVideo] = useState(false)\n  const [textString, setTextString] = useState(\"\")\n\n  const faceData = useRef([])\n  const faceTriggered = useRef(false)\n  const videoRef = useRef()\n  const videoHeight = 480\n  const videoWidth = 640\n  const canvasRef = useRef()\n  const audioRef = useRef(null)\n\n  const handleInputText = (e) => {\n    const { value } = e.target\n    setTextString(value)\n  }\n\n  const handleOnKeyDown = (e) => {\n    if (e.key === \"Enter\") {\n      console.log(\"textString :>> \", textString)\n      myFunc(textString, { api_body: { keyword: \"\" } }, 4)\n      setTextString(\"\")\n    }\n  }\n\n  const startVideo = () => {\n    setCaptureVideo(true)\n    navigator.mediaDevices\n      .getUserMedia({ video: { width: 300 } })\n      .then((stream) => {\n        let video = videoRef.current\n        video.srcObject = stream\n        video.play()\n      })\n      .catch((err) => {\n        console.error(\"error:\", err)\n      })\n  }\n\n  const handleVideoOnPlay = () => {\n    setInterval(async () => {\n      if (canvasRef && canvasRef.current) {\n        canvasRef.current.innerHTML = faceapi.createCanvasFromMedia(\n          videoRef.current\n        )\n        const displaySize = {\n          width: videoWidth,\n          height: videoHeight,\n        }\n\n        faceapi.matchDimensions(canvasRef.current, displaySize)\n\n        const detections = await faceapi\n          .detectAllFaces(\n            videoRef.current,\n            new faceapi.TinyFaceDetectorOptions()\n          )\n          .withFaceLandmarks()\n          .withFaceExpressions()\n\n        const resizedDetections = faceapi.resizeResults(detections, displaySize)\n\n        if (resizedDetections.length > 0) {\n          faceData.current = resizedDetections\n          if (!faceTriggered.current && face_recon) {\n            myFunc(\"\", { api_body: { keyword: \"\" } }, 2)\n            faceTriggered.current = true\n          }\n        } else {\n          faceTimer && clearTimeout(faceTimer)\n          setTimeout(() => {\n            faceData.current = []\n          }, 1000)\n        }\n\n        if (resizedDetections.length > 0 && !firstFace) {\n          firstFace = true\n          if (kwargs.hello_audio) {\n            const audio = new Audio(kwargs.hello_audio)\n            audio.play()\n          }\n        }\n\n        canvasRef &&\n          canvasRef.current &&\n          canvasRef.current\n            .getContext(\"2d\")\n            .clearRect(0, 0, videoWidth, videoHeight)\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawDetections(canvasRef.current, resizedDetections)\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawFaceLandmarks(canvasRef.current, resizedDetections)\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawFaceExpressions(canvasRef.current, resizedDetections)\n      }\n    }, 300)\n  }\n\n  const closeWebcam = () => {\n    videoRef.current.pause()\n    videoRef.current.srcObject.getTracks()[0].stop()\n    setCaptureVideo(false)\n  }\n  const testFunc = async () => {\n    const audio = new Audio(\"./test_audio.mp3s\")\n    console.log(audio.play())\n    const response = await axios.post(\n      \"http://192.168.143.97:8000/api/data/voiceGPT\",\n      {\n        api_key: \"sdf\",\n        text: \"text\",\n        self_image: \"something\",\n      }\n    )\n    console.log(\"response :>> \", response)\n  }\n\n  const myFunc = async (ret, command, type) => {\n    setMessage(` (${command[\"api_body\"][\"keyword\"]}) ${ret},`)\n    const text = [...g_anwers, { user: ret }]\n    setAnswers([...text])\n    try {\n      console.log(\"api call on listen...\", command)\n      const body = {\n        tigger_type: type,\n        api_key: \"api_key\",\n        text: text,\n        self_image: imageSrc,\n        face_data: faceData.current,\n      }\n      console.log(\"api\")\n      const { data } = await axios.post(api, body)\n      console.log(\"data :>> \", data, body)\n      data[\"self_image\"] && setImageSrc(data[\"self_image\"])\n      if (data[\"audio_path\"]) {\n        if (audioRef.current) {\n          audioRef.current.pause(); // Pause existing playback if any\n        }\n        audioRef.current = new Audio(data[\"audio_path\"]);\n        audioRef.current.play();\n        \n        // const audio = new Audio(data[\"audio_path\"]);\n        // audio.play();\n  \n        audioRef.current.onended = () => {\n          console.log(\"Audio playback finished.\");\n  \n          if (data[\"listen_after_reply\"]) {\n            setListenAfterReply(data[\"listen_after_reply\"]);\n          }\n  \n          setAnswers(data[\"text\"]);\n          g_anwers = [...data[\"text\"]];\n    \n          if (data[\"page_direct\"] === true) {\n            console.log(\"api has page direct\", data[\"page_direct\"]);\n            window.location.reload();\n          }\n        };\n      } else {\n        if (data[\"listen_after_reply\"]) {\n          setListenAfterReply(data[\"listen_after_reply\"]);\n        }\n  \n        setAnswers(data[\"text\"]);\n        g_anwers = [...data[\"text\"]];\n    \n        if (data[\"page_direct\"] === true) {\n          console.log(\"api has page direct\", data[\"page_direct\"]);\n          window.location.reload();\n        }\n      }\n    } catch (error) {\n      console.log(\"api call on listen failded!\", error)\n    }\n  }\n\n  // const commands = useMemo(() => {\n  //   return kwargs[\"commands\"].map((command) => ({\n  //     command: command[\"keywords\"],\n  //     callback: (ret) => {\n  //       timer && clearTimeout(timer)\n  //       timer = setTimeout(() => myFunc(ret, command, 1), 1000)\n  //     },\n  //     matchInterim: true,\n  //   }))\n  // }, [kwargs.commands])\n  // const commands = [\n  //   {\n  //     command: \"I would like to order *\",\n  //     callback: (food) => setMessage(`Your order is for: ${food}`),\n  //     matchInterim: true,\n  //   },\n  //   {\n  //     command: \"The weather is :condition today\",\n  //     callback: (condition) => setMessage(`Today, the weather is ${condition}`),\n  //   },\n  //   {\n  //     command: [\"Hey foots\", \"Hey foods\"],\n  //     callback: ({ command }) => setMessage(`Hi there! You said: \"${command}\"`),\n  //     matchInterim: true,\n  //   },\n  //   {\n  //     command: \"Beijing\",\n  //     callback: (command, spokenPhrase, similarityRatio) =>\n  //       setMessage(\n  //         `${command} and ${spokenPhrase} are ${similarityRatio * 100}% similar`\n  //       ),\n  //     // If the spokenPhrase is \"Benji\", the message would be \"Beijing and Benji are 40% similar\"\n  //     isFuzzyMatch: true,\n  //     fuzzyMatchingThreshold: 0.2,\n  //   },\n  //   {\n  //     command: [\"eat\", \"sleep\", \"leave\"],\n  //     callback: (command) => setMessage(`Best matching command: ${command}`),\n  //     isFuzzyMatch: true,\n  //     fuzzyMatchingThreshold: 0.2,\n  //     bestMatchOnly: true,\n  //   },\n  //   {\n  //     command: \"clear\",\n  //     callback: ({ resetTranscript }) => resetTranscript(),\n  //     matchInterim: true,\n  //   },\n  // ]\n\n  const listenContinuously = () =>\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"en-GB\",\n    })\n  const listenContinuouslyInChinese = () =>\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"zh-CN\",\n    })\n  const listenOnce = () =>\n    SpeechRecognition.startListening({ continuous: false })\n\n  useEffect(() => Streamlit.setFrameHeight())\n\n  useEffect(() => {}, [props])\n\n  useEffect(() => {\n    const loadModels = async () => {\n      const MODEL_URL = process.env.PUBLIC_URL + \"/models\"\n\n      Promise.all([\n        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),\n        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),\n      ]).then(setModelsLoaded(true))\n    }\n    loadModels()\n    const interval = setInterval(() => {\n      console.log(\"faceData.current :>> \", faceData.current)\n    }, 3000)\n    return () => clearInterval(interval)\n  }, [])\n\n  return (\n    <>\n      <div className=\"p-2\">\n        <div>\n          <img src={imageSrc} height={height || 100} width={width || 100} />\n        </div>\n        <div className=\"p-2\">\n          <Dictaphone\n            commands={commands}\n            myFunc={myFunc}\n            listenAfterRelpy={listenAfterRelpy}\n            noResponseTime={no_response_time}\n            show_conversation={show_conversation}\n          />\n        </div>\n        <div className=\"form-group\">\n          <button className=\"btn btn-primary\" onClick={listenContinuously}>\n            Listen continuously\n          </button>\n        </div>\n        {input_text && (\n          <div className=\"form-group\">\n            <input\n              className=\"form-control\"\n              type=\"text\"\n              placeholder=\"Chat with chatGPT\"\n              value={textString}\n              onChange={handleInputText}\n              onKeyDown={handleOnKeyDown}\n            />\n          </div>\n        )}\n        {show_conversation === true && (\n          <>\n            <div> You: {message}</div>\n            {answers.map((answer, idx) => (\n              <div key={idx}>\n                <div>-user: {answer.user}</div>\n                <div>-resp: {answer.resp ? answer.resp : \"thinking...\"}</div>\n              </div>\n            ))}\n          </>\n        )}\n      </div>\n      <div>\n        {/* <button onClick={listenOnce}>Listen Once</button> */}\n        {/* <button onClick={listenContinuouslyInChinese}></button> */}\n        {/* <button onClick={SpeechRecognition.stopListening}>Stop</button> */}\n        {/* <button onClick={testFunc}>test</button> */}\n      </div>\n      <div>\n        {face_recon && (\n          <div style={{ textAlign: \"center\", padding: \"10px\" }}>\n            {captureVideo && modelsLoaded ? (\n              <button\n                onClick={closeWebcam}\n                style={{\n                  cursor: \"pointer\",\n                  backgroundColor: \"green\",\n                  color: \"white\",\n                  padding: \"15px\",\n                  fontSize: \"25px\",\n                  border: \"none\",\n                  borderRadius: \"10px\",\n                }}\n              >\n                Close Webcam\n              </button>\n            ) : (\n              <button\n                onClick={startVideo}\n                style={{\n                  cursor: \"pointer\",\n                  backgroundColor: \"green\",\n                  color: \"white\",\n                  padding: \"15px\",\n                  fontSize: \"25px\",\n                  border: \"none\",\n                  borderRadius: \"10px\",\n                }}\n              >\n                Open Webcam\n              </button>\n            )}\n          </div>\n        )}\n        {captureVideo ? (\n          modelsLoaded ? (\n            <div>\n              <div\n                style={{\n                  display: \"flex\",\n                  justifyContent: \"center\",\n                  padding: \"10px\",\n                  position: show_video ? \"\" : \"absolute\",\n                  opacity: show_video ? 1 : 0.3,\n                }}\n              >\n                <video\n                  ref={videoRef}\n                  height={videoHeight}\n                  width={videoWidth}\n                  onPlay={handleVideoOnPlay}\n                  style={{ borderRadius: \"10px\" }}\n                />\n                <canvas ref={canvasRef} style={{ position: \"absolute\" }} />\n              </div>\n            </div>\n          ) : (\n            <div>loading...</div>\n          )\n        ) : (\n          <></>\n        )}\n      </div>\n    </>\n  )\n}\n\nexport default CustomVoiceGPT\n","import React, { useEffect, useState } from \"react\"\nimport {\n  ComponentProps,\n  Streamlit,\n  withStreamlitConnection,\n} from \"streamlit-component-lib\"\nimport VoiceGPT from \"./VoiceGPT.jsx\"\n\nconst Main = (props: ComponentProps) => {\n  const { api, kwargs } = props.args\n  useEffect(() => Streamlit.setFrameHeight())\n  return (\n    <>\n      <VoiceGPT api={api} kwargs={kwargs} />\n    </>\n  )\n}\n\nexport default withStreamlitConnection(Main)\n","import React from \"react\"\nimport ReactDOM from \"react-dom\"\nimport Main from \"./Main\"\n// Lots of import to define a Styletron engine and load the light theme of baseui\nimport { Client as Styletron } from \"styletron-engine-atomic\"\nimport { Provider as StyletronProvider } from \"styletron-react\"\nimport { ThemeProvider, LightTheme } from \"baseui\"\n\nconst engine = new Styletron()\n\n// Wrap your CustomSlider with the baseui them\nReactDOM.render(\n  <React.StrictMode>\n    <StyletronProvider value={engine}>\n      <ThemeProvider theme={LightTheme}>\n        <Main />\n      </ThemeProvider>\n    </StyletronProvider>\n  </React.StrictMode>,\n  document.getElementById(\"root\")\n)\n"],"sourceRoot":""}