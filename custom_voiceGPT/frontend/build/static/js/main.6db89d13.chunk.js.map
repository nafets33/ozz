{"version":3,"sources":["../node_modules/@vladmandic/face-api/dist sync","Dictaphone.jsx","VoiceGPT.jsx","Main.tsx","index.tsx"],"names":["webpackEmptyContext","req","e","Error","code","keys","resolve","module","exports","id","Dictaphone","_ref","commands","myFunc","listenAfterReply","noResponseTime","show_conversation","apiInProgress","transcribing","setTranscribing","useState","clearTranscriptOnListen","setClearTranscriptOnListen","finalTranscript","resetTranscript","listening","browserSupportsSpeechRecognition","isMicrophoneAvailable","useSpeechRecognition","prevScript","setPrevScript","useEffect","console","log","split","length","timer","setTimeout","i","keywords","api_body","j","keyword","RegExp","isKeywordFound","search","clearTimeout","React","createElement","Fragment","style","display","flexDirection","g_anwers","firstFace","CustomVoiceGPT","props","api","kwargs","height","width","show_video","input_text","no_response_time","face_recon","api_key","refresh_ask","before_trigger","api_audio","client_user","imageSrc","setImageSrc","self_image","message","setMessage","answers","setAnswers","setListenAfterReply","modelsLoaded","setModelsLoaded","captureVideo","setCaptureVideo","textString","setTextString","setApiInProgress","faceData","useRef","faceTriggered","videoRef","canvasRef","audioRef","isListening","setIsListening","isGreenLightOn","setIsGreenLightOn","async","ret","command","type","concat","text","user","stopListening","body","tigger_type","face_data","current","data","axios","post","pause","apiUrlWithFileName","Audio","play","Promise","onended","window","location","href","listenContinuously","error","Streamlit","setFrameHeight","intervalId","setInterval","SpeechRecognition","browserSupportsContinuousListening","clearInterval","startListening","continuous","language","all","faceapi","tinyFaceDetector","loadFromUri","process","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","ageGenderNet","then","loadModels","interval","className","toLowerCase","endsWith","controls","autoPlay","loop","muted","src","position","top","left","background","animation","onClick","placeholder","value","onChange","target","onKeyDown","key","map","answer","idx","resp","textAlign","padding","closeWebcam","srcObject","getTracks","stop","cursor","backgroundColor","color","fontSize","border","borderRadius","startVideo","navigator","mediaDevices","getUserMedia","video","stream","catch","err","justifyContent","opacity","ref","onPlay","handleVideoOnPlay","innerHTML","displaySize","detections","withFaceLandmarks","withFaceExpressions","resizedDetections","hello_audio","getContext","clearRect","drawDetections","drawFaceLandmarks","drawFaceExpressions","withStreamlitConnection","args","VoiceGPT","engine","Styletron","ReactDOM","render","StrictMode","StyletronProvider","ThemeProvider","theme","LightTheme","Main","document","getElementById"],"mappings":"0HAAA,SAASA,EAAoBC,GAC5B,IAAIC,EAAI,IAAIC,MAAM,uBAAyBF,EAAM,KAEjD,MADAC,EAAEE,KAAO,mBACHF,EAEPF,EAAoBK,KAAO,WAAa,MAAO,IAC/CL,EAAoBM,QAAUN,EAC9BO,EAAOC,QAAUR,EACjBA,EAAoBS,GAAK,I,+IC4EVC,MAjFIC,IAOZ,IAPa,SAClBC,EAAQ,OACRC,EAAM,iBACNC,GAAmB,EAAK,eACxBC,EAAiB,EAAC,kBAClBC,GAAoB,EAAI,cACxBC,GAAgB,GACjBN,EACC,MAAOO,EAAcC,GAAmBC,oBAAS,IAC1CC,EAAyBC,GAA8BF,oBAAS,IACjE,gBAAEG,EAAe,gBAAEC,EAAe,UAAEC,EAAS,iCAAEC,EAAgC,sBAAEC,GAA0BC,+BAAqB,CAAEV,eAAcG,6BAC/IQ,EAAYC,GAAiBV,mBAAS,IAiD7C,OA/CAW,oBAAU,KACR,GAAwB,KAApBR,EAAwB,CAS1B,GARAS,QAAQC,IAAI,oBAAqBV,GACjCS,QAAQC,IAAI,aAAcR,GAG1BO,QAAQC,IAAI,oBAAqBnB,GAI7BS,EAAgBW,MAAM,KAAKC,OAAS,GAGtC,OAFAH,QAAQC,IAAI,+CACZT,IAKFM,EAAcP,GAGd,MAAMa,EAAQC,WAAW,KACvB,IAAK,IAAIC,EAAI,EAAGA,EAAI1B,EAASuB,OAAQG,IAAK,CACxC,MAAM,SAAEC,EAAQ,SAAEC,GAAa5B,EAAS0B,GACxC,IAAK,IAAIG,EAAI,EAAGA,EAAIF,EAASJ,OAAQM,IAAK,CACxC,MAAMC,EAAU,IAAIC,OAAOJ,EAASE,GAAI,KAClCG,GAAsD,IAArCrB,EAAgBsB,OAAOH,GAE9C,GADAV,QAAQC,IAAI,oBAAqBnB,IAC5B8B,GAAkB9B,KAAsBG,EAO3C,OANIH,EACFD,EAAOU,EAAiB,CAAEiB,SAAU,CAAEE,QAAS,KAAQ,GAC9CE,GACT/B,EAAOU,EAAiBX,EAAS0B,GAAI,QAEvCd,KAMNQ,QAAQC,IAAI,gDACM,IAAjBlB,GAEH,MAAO,IAAM+B,aAAaV,KAE3B,CAACb,EAAiBT,EAAkBF,EAAUG,EAAgBS,EAAiBP,IAG7ES,EAIAC,EAKHoB,IAAAC,cAAAD,IAAAE,SAAA,KACGjC,GACC+B,IAAAC,cAAA,OAAKE,MAAO,CAAEC,QAAS,OAAQC,cAAe,WAC5CL,IAAAC,cAAA,YAAM,aAAWnB,GACjBkB,IAAAC,cAAA,YAAM,cAAYvB,EAAY,KAAO,OACrCsB,IAAAC,cAAA,YAAM,+BAA6B3B,EAA0B,KAAO,SATnE0B,IAAAC,cAAA,YAAM,yCAJND,IAAAC,cAAA,YAAM,uB,OCzDjB,IAEIK,EAAW,GACXC,GAAY,EAuaDC,MAraSC,IACtB,MAAM,IAAEC,EAAG,OAAEC,EAAS,IAAOF,GACvB,SACJ5C,EAAQ,OACR+C,EAAM,MACNC,EAAK,kBACL5C,EAAiB,WACjB6C,EAAU,WACVC,EAAU,iBACVC,EAAgB,WAChBC,EAAU,QACVC,EAAO,YACPC,EAAW,eACXC,EAAc,UACdC,EAAS,YACTC,GACEX,GACGY,EAAUC,GAAenD,mBAASsC,EAAOc,aACzCC,EAASC,GAActD,mBAAS,KAChCuD,EAASC,GAAcxD,mBAAS,KAChCN,EAAkB+D,GAAuBzD,oBAAS,IAClD0D,EAAcC,GAAmB3D,oBAAS,IAC1C4D,EAAcC,GAAmB7D,oBAAS,IAC1C8D,EAAYC,GAAiB/D,mBAAS,KACtCH,EAAemE,GAAoBhE,oBAAS,GAE7CiE,EAAWC,iBAAO,IAClBC,EAAgBD,kBAAO,GACvBE,EAAWF,mBAGXG,EAAYH,mBACZI,EAAWJ,iBAAO,OAEjBK,EAAaC,GAAkBxE,oBAAS,IACxCyE,EAAgBC,GAAqB1E,oBAAS,GAgH/CP,EAASkF,MAAOC,EAAKC,EAASC,KAClCxB,EAAW,KAADyB,OAAMF,EAAkB,SAAW,QAAC,MAAAE,OAAKH,EAAG,MACtD,MAAMI,EAAO,IAAI/C,EAAU,CAAEgD,KAAML,IACnCpB,EAAW,IAAIwB,IACf,IACEpE,QAAQC,IAAI,wBAAyBgE,GACrCb,GAAiB,GACjBkB,IAEA,MAAMC,EAAO,CACXC,YAAaN,EACbjC,QAASA,EACTmC,KAAMA,EACN5B,WAAYF,EACZmC,UAAWpB,EAASqB,QACpBxC,YAAaA,EACbG,YAAaA,GAEfrC,QAAQC,IAAI,OACZ,MAAM,KAAE0E,SAAeC,IAAMC,KAAKpD,EAAK8C,GACvCvE,QAAQC,IAAI,YAAa0E,EAAMJ,GAC/BI,EAAiB,YAAKpC,EAAYoC,EAAiB,YAC/CjB,EAASgB,SACXhB,EAASgB,QAAQI,QAInB,MAAMC,EAAkB,GAAAZ,OAAM/B,GAAS+B,OAAGQ,EAAiB,YAC3DjB,EAASgB,QAAU,IAAIM,MAAMD,GAC7BrB,EAASgB,QAAQO,aAGX,IAAIC,QAAS5G,IACjBoF,EAASgB,QAAQS,QAAU,KACzBnF,QAAQC,IAAI,4BACZ3B,OAIJ0B,QAAQC,IAAI,kCACZ2C,EAAW+B,EAAW,MACtBtD,EAAW,IAAIsD,EAAW,MAE1B9B,EAAoB8B,EAAyB,oBAC7C3E,QAAQC,IAAI,qBAAsB0E,EAAyB,qBAE/B,IAAxBA,EAAkB,aAAuC,OAAxBA,EAAkB,cACrD3E,QAAQC,IAAI,sBAAuB0E,EAAkB,aAErDS,OAAOC,SAASC,KAAOX,EAAkB,aAG3CY,IACAnC,GAAiB,GAEjB,MAAOoC,OACPxF,QAAQC,IAAI,6BAA8BuF,OAC1CpC,GAAiB,KAIrBrD,oBAAU,KACR0F,IAAUC,iBAGV,MAAMC,EAAaC,YAAY,KACxBC,IAAkBC,uCAErB9F,QAAQC,IAAI,iCAAkCuF,OAC9CD,MAED,KAEH,MAAO,KACLQ,cAAcJ,KAEf,IAEH,MASMrB,EAAgBA,KACpBuB,IAAkBvB,iBAMdiB,EAAqBA,IACzBM,IAAkBG,eAAe,CAC/BC,YAAY,EACZC,SAAU,UA8Bd,OAnBAnG,oBAAU,KACWgE,WAGjBmB,QAAQiB,IAAI,CACVC,IAAaC,iBAAiBC,YAHdC,YAIhBH,IAAaI,kBAAkBF,YAJfC,YAKhBH,IAAaK,mBAAmBH,YALhBC,YAMhBH,IAAaM,kBAAkBJ,YANfC,YAOhBH,IAAaO,aAAaL,YAPVC,cAQfK,KAAK,IAAM7D,GAAgB,KAEhC8D,GACA,MAAMC,EAAWlB,YAAY,KAC3B5F,QAAQC,IAAI,wBAAyBoD,EAASqB,UAC7C,KACH,MAAO,IAAMqB,cAAce,IAC1B,IAGD/F,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAA,OAAK+F,UAAU,OACfhG,IAAAC,cAAA,WACKsB,GAAYA,EAAS0E,cAAcC,SAAS,QAC3ClG,IAAAC,cAAA,SACEW,OAAQA,GAAU,IAClBC,MAAOA,GAAS,IAChBsF,UAAQ,EACRC,UAAU,EACVC,MAAM,EACNC,OAAK,GAELtG,IAAAC,cAAA,UAAQsG,IAAKhF,EAAU4B,KAAK,cAAc,gDAI5CnD,IAAAC,cAAA,OAAKsG,IAAKhF,EAAUX,OAAQA,GAAU,IAAKC,MAAOA,GAAS,MAG5D3C,GACC8B,IAAAC,cAAA,OACEE,MAAO,CACLqG,SAAU,WACVC,IAAK,OACLC,KAAM,IACN7F,MAAO,OACPD,OAAQ,MACR+F,WAAY,wDACZC,UAAW,4BAKnB5G,IAAAC,cAAA,OAAK+F,UAAU,OACbhG,IAAAC,cAACtC,EAAU,CACTE,SAAUA,EACVC,OAAQA,EACRC,iBAAkBA,EAClBC,eAAgBgD,EAChB/C,kBAAmBA,EACnBC,cAAeA,KAGnB8B,IAAAC,cAAA,OAAK+F,UAAU,cACbhG,IAAAC,cAAA,UAAQ+F,UAAU,kBAAkBa,QAASrC,GAAoB,wBAIlEzD,GACCf,IAAAC,cAAA,OAAK+F,UAAU,cACbhG,IAAAC,cAAA,SACE+F,UAAU,eACV7C,KAAK,OACL2D,YAAY,kBACZC,MAAO5E,EACP6E,SAzRa7J,IACvB,MAAM,MAAE4J,GAAU5J,EAAE8J,OACpB7E,EAAc2E,IAwRJG,UArRa/J,IACT,UAAVA,EAAEgK,MACJlI,QAAQC,IAAI,kBAAmBiD,GAC/BrE,EAAOqE,EAAY,CAAE1C,SAAU,CAAEE,QAAS,KAAQ,GAClDyC,EAAc,UAqRW,IAAtBnE,GACC+B,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAA,WAAK,SAAOyB,GACXE,EAAQwF,IAAI,CAACC,EAAQC,IACpBtH,IAAAC,cAAA,OAAKkH,IAAKG,GACRtH,IAAAC,cAAA,WAAK,UAAQoH,EAAO/D,MACpBtD,IAAAC,cAAA,WAAK,UACKoH,EAAOE,KAAOF,EAAOE,KAAO,mBAOhDvH,IAAAC,cAAA,YAGAD,IAAAC,cAAA,WACGgB,GACCjB,IAAAC,cAAA,OAAKE,MAAO,CAAEqH,UAAW,SAAUC,QAAS,SACzCxF,GAAgBF,EACf/B,IAAAC,cAAA,UACE4G,QAxNMa,KAClBjF,EAASkB,QAAQI,QACjBtB,EAASkB,QAAQgE,UAAUC,YAAY,GAAGC,OAC1C3F,GAAgB,IAsNJ/B,MAAO,CACL2H,OAAQ,UACRC,gBAAiB,QACjBC,MAAO,QACPP,QAAS,OACTQ,SAAU,OACVC,OAAQ,OACRC,aAAc,SAEjB,gBAIDnI,IAAAC,cAAA,UACE4G,QAtTKuB,KACjBlG,GAAgB,GAChBmG,UAAUC,aACPC,aAAa,CAAEC,MAAO,CAAE3H,MAAO,OAC/BgF,KAAM4C,IACL,IAAID,EAAQ/F,EAASkB,QACrB6E,EAAMb,UAAYc,EAClBD,EAAMtE,SAEPwE,MAAOC,IACN1J,QAAQwF,MAAM,SAAUkE,MA6ShBxI,MAAO,CACL2H,OAAQ,UACRC,gBAAiB,QACjBC,MAAO,QACPP,QAAS,OACTQ,SAAU,OACVC,OAAQ,OACRC,aAAc,SAEjB,gBAMNlG,EACCF,EACE/B,IAAAC,cAAA,WACED,IAAAC,cAAA,OACEE,MAAO,CACLC,QAAS,OACTwI,eAAgB,SAChBnB,QAAS,OACTjB,SAAU1F,EAAa,GAAK,WAC5B+H,QAAS/H,EAAa,EAAI,KAG5Bd,IAAAC,cAAA,SACE6I,IAAKrG,EACL7B,OArXI,IAsXJC,MArXG,IAsXHkI,OAxUUC,KACxBnE,YAAY7B,UACV,GAAIN,GAAaA,EAAUiB,QAAS,CAClCjB,EAAUiB,QAAQsF,UAAY5D,IAC5B5C,EAASkB,SAEX,MAAMuF,EAAc,CAClBrI,MArDW,IAsDXD,OAvDY,KA0DdyE,IAAwB3C,EAAUiB,QAASuF,GAE3C,MAAMC,QAAmB9D,IAErB5C,EAASkB,QACT,IAAI0B,KAEL+D,oBACAC,sBAEGC,EAAoBjE,IAAsB8D,EAAYD,GAe5D,GAbII,EAAkBlK,OAAS,GAC7BkD,EAASqB,QAAU2F,GACd9G,EAAcmB,SAAW1C,IAC5BnD,EAAO,GAAI,CAAE2B,SAAU,CAAEE,QAAS,KAAQ,GAC1C6C,EAAcmB,SAAU,IAI1BrE,WAAW,KACTgD,EAASqB,QAAU,IAClB,KAGD2F,EAAkBlK,OAAS,IAAMmB,IACnCA,GAAY,EACRI,EAAO4I,aAAa,CACR,IAAItF,MAAMtD,EAAO4I,aACzBrF,OAIVxB,GACEA,EAAUiB,SACVjB,EAAUiB,QACP6F,WAAW,MACXC,UAAU,EAAG,EA9FL,IADC,KAgGd/G,GACEA,EAAUiB,SACV0B,IAAaqE,eAAehH,EAAUiB,QAAS2F,GACjD5G,GACEA,EAAUiB,SACV0B,IAAasE,kBAAkBjH,EAAUiB,QAAS2F,GACpD5G,GACEA,EAAUiB,SACV0B,IAAauE,oBACXlH,EAAUiB,QACV2F,KAGL,MA2QWnJ,MAAO,CAAEgI,aAAc,UAEzBnI,IAAAC,cAAA,UAAQ6I,IAAKpG,EAAWvC,MAAO,CAAEqG,SAAU,gBAI/CxG,IAAAC,cAAA,WAAK,cAGPD,IAAAC,cAAAD,IAAAE,SAAA,SCxZK2J,kBAVDpJ,IACZ,MAAM,IAAEC,EAAG,OAAEC,GAAWF,EAAMqJ,KAE9B,OADA9K,oBAAU,IAAM0F,IAAUC,kBAExB3E,IAAAC,cAAAD,IAAAE,SAAA,KACEF,IAAAC,cAAC8J,EAAQ,CAACrJ,IAAKA,EAAKC,OAAQA,O,gCCLlC,MAAMqJ,EAAS,IAAIC,IAGnBC,IAASC,OACPnK,IAAAC,cAACD,IAAMoK,WAAU,KACfpK,IAAAC,cAACoK,IAAiB,CAACtD,MAAOiD,GACxBhK,IAAAC,cAACqK,IAAa,CAACC,MAAOC,KACpBxK,IAAAC,cAACwK,EAAI,SAIXC,SAASC,eAAe,W","file":"static/js/main.6db89d13.chunk.js","sourcesContent":["function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = function() { return []; };\nwebpackEmptyContext.resolve = webpackEmptyContext;\nmodule.exports = webpackEmptyContext;\nwebpackEmptyContext.id = 20;","import React, { useState, useEffect } from \"react\";\nimport SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';\n\nconst Dictaphone = ({\n  commands,\n  myFunc,\n  listenAfterReply = false,\n  noResponseTime = 1,\n  show_conversation = true,\n  apiInProgress = false, // Receive apiInProgress as a prop\n}) => {\n  const [transcribing, setTranscribing] = useState(true);\n  const [clearTranscriptOnListen, setClearTranscriptOnListen] = useState(true);\n  const { finalTranscript, resetTranscript, listening, browserSupportsSpeechRecognition, isMicrophoneAvailable } = useSpeechRecognition({ transcribing, clearTranscriptOnListen });\n  const [prevScript, setPrevScript] = useState(\"\");\n\n  useEffect(() => {\n    if (finalTranscript !== \"\") {\n      console.log(\"Got final result:\", finalTranscript);\n      console.log(\"listening?\", listening);\n\n      // Add logs to check the conditions\n      console.log(\"listenAfterReply:\", listenAfterReply);\n      // console.log(\"Number of words:\", finalTranscript.split(\" \").length);\n\n      // Clear the previous script if a keyword is found or if the transcript exceeds 89 words\n      if (finalTranscript.split(\" \").length > 89) {\n        console.log(\"Transcript exceeds 89 words. Clearing.\");\n        resetTranscript();\n        return;\n      }\n\n      // Set the previous script\n      setPrevScript(finalTranscript);\n\n      // Start the timer to check for keywords after a pause\n      const timer = setTimeout(() => {\n        for (let i = 0; i < commands.length; i++) {\n          const { keywords, api_body } = commands[i];\n          for (let j = 0; j < keywords.length; j++) {\n            const keyword = new RegExp(keywords[j], \"i\");\n            const isKeywordFound = finalTranscript.search(keyword) !== -1;\n            console.log(\"listenAfterReply:\", listenAfterReply);\n            if ((isKeywordFound || listenAfterReply) && !apiInProgress) {\n              if (listenAfterReply) {\n                myFunc(finalTranscript, { api_body: { keyword: \"\" } }, 3);\n              } else if (isKeywordFound) {\n                myFunc(finalTranscript, commands[i], 1);\n              }\n              resetTranscript();\n              return;\n            }\n          }\n        }\n        // Waiting for a keyword or API is in progress\n        console.log(\"Waiting for a keyword or API is in progress\");\n      }, noResponseTime * 1000);\n\n      return () => clearTimeout(timer); // Clear the timer on component unmount or when useEffect runs again\n    }\n  }, [finalTranscript, listenAfterReply, commands, noResponseTime, resetTranscript, apiInProgress]);\n\n\n  if (!browserSupportsSpeechRecognition) {\n    return <span>No browser support</span>;\n  }\n\n  if (!isMicrophoneAvailable) {\n    return <span>Please allow access to the microphone</span>;\n  }\n\n  return (\n    <>\n      {show_conversation && (\n        <div style={{ display: \"flex\", flexDirection: \"column\" }}>\n          <span>You said: {prevScript}</span>\n          <span>Listening: {listening ? \"on\" : \"off\"}</span>\n          <span>Clear Transcript On Listen: {clearTranscriptOnListen ? \"on\" : \"off\"}</span>\n        </div>\n      )}\n    </>\n  );\n};\n\nexport default Dictaphone;\n","import React, { useState, useEffect, useRef } from \"react\";\nimport axios from \"axios\";\nimport { Streamlit } from \"streamlit-component-lib\";\nimport SpeechRecognition from \"react-speech-recognition\";\nimport Dictaphone from \"./Dictaphone\";\nimport * as faceapi from \"@vladmandic/face-api\";\n\nlet timer = null;\nlet faceTimer = null;\nlet g_anwers = [];\nlet firstFace = false;\n\nconst CustomVoiceGPT = (props) => {\n  const { api, kwargs = {} } = props;\n  const {\n    commands,\n    height,\n    width,\n    show_conversation,\n    show_video,\n    input_text,\n    no_response_time,\n    face_recon,\n    api_key,\n    refresh_ask,\n    before_trigger,\n    api_audio,\n    client_user,\n  } = kwargs;\n  const [imageSrc, setImageSrc] = useState(kwargs.self_image);\n  const [message, setMessage] = useState(\"\");\n  const [answers, setAnswers] = useState([]);\n  const [listenAfterReply, setListenAfterReply] = useState(false);\n  const [modelsLoaded, setModelsLoaded] = useState(false);\n  const [captureVideo, setCaptureVideo] = useState(false);\n  const [textString, setTextString] = useState(\"\");\n  const [apiInProgress, setApiInProgress] = useState(false); // Added state for API in progress\n\n  const faceData = useRef([]);\n  const faceTriggered = useRef(false);\n  const videoRef = useRef();\n  const videoHeight = 480;\n  const videoWidth = 640;\n  const canvasRef = useRef();\n  const audioRef = useRef(null);\n\n  const [isListening, setIsListening] = useState(true);\n  const [isGreenLightOn, setIsGreenLightOn] = useState(false);\n\n\n  // ... (other code)\n\n  const checkListeningStatus = () => {\n    // Check if continuous listening is active\n    if (!SpeechRecognition.browserSupportsContinuousListening()) {\n      // If not, restart continuous listening\n      startContinuousListening();\n    }\n  };\n\n\n  const handleInputText = (e) => {\n    const { value } = e.target;\n    setTextString(value);\n  };\n\n  const handleOnKeyDown = (e) => {\n    if (e.key === \"Enter\") {\n      console.log(\"textString :>> \", textString);\n      myFunc(textString, { api_body: { keyword: \"\" } }, 4);\n      setTextString(\"\");\n    }\n  };\n\n  const startVideo = () => {\n    setCaptureVideo(true);\n    navigator.mediaDevices\n      .getUserMedia({ video: { width: 300 } })\n      .then((stream) => {\n        let video = videoRef.current;\n        video.srcObject = stream;\n        video.play();\n      })\n      .catch((err) => {\n        console.error(\"error:\", err);\n      });\n  };\n\n  const handleVideoOnPlay = () => {\n    setInterval(async () => {\n      if (canvasRef && canvasRef.current) {\n        canvasRef.current.innerHTML = faceapi.createCanvasFromMedia(\n          videoRef.current\n        );\n        const displaySize = {\n          width: videoWidth,\n          height: videoHeight,\n        };\n\n        faceapi.matchDimensions(canvasRef.current, displaySize);\n\n        const detections = await faceapi\n          .detectAllFaces(\n            videoRef.current,\n            new faceapi.TinyFaceDetectorOptions()\n          )\n          .withFaceLandmarks()\n          .withFaceExpressions();\n\n        const resizedDetections = faceapi.resizeResults(detections, displaySize);\n\n        if (resizedDetections.length > 0) {\n          faceData.current = resizedDetections;\n          if (!faceTriggered.current && face_recon) {\n            myFunc(\"\", { api_body: { keyword: \"\" } }, 2);\n            faceTriggered.current = true;\n          }\n        } else {\n          faceTimer && clearTimeout(faceTimer);\n          setTimeout(() => {\n            faceData.current = [];\n          }, 1000);\n        }\n\n        if (resizedDetections.length > 0 && !firstFace) {\n          firstFace = true;\n          if (kwargs.hello_audio) {\n            const audio = new Audio(kwargs.hello_audio);\n            audio.play();\n          }\n        }\n\n        canvasRef &&\n          canvasRef.current &&\n          canvasRef.current\n            .getContext(\"2d\")\n            .clearRect(0, 0, videoWidth, videoHeight);\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawDetections(canvasRef.current, resizedDetections);\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawFaceLandmarks(canvasRef.current, resizedDetections);\n        canvasRef &&\n          canvasRef.current &&\n          faceapi.draw.drawFaceExpressions(\n            canvasRef.current,\n            resizedDetections\n          );\n      }\n    }, 300);\n  };\n\n  const closeWebcam = () => {\n    videoRef.current.pause();\n    videoRef.current.srcObject.getTracks()[0].stop();\n    setCaptureVideo(false);\n  };\n\n  const myFunc = async (ret, command, type) => {\n    setMessage(` (${command[\"api_body\"][\"keyword\"]}) ${ret},`);\n    const text = [...g_anwers, { user: ret }];\n    setAnswers([...text]);\n    try {\n      console.log(\"api call on listen...\", command);\n      setApiInProgress(true); // Set API in progress to true\n      stopListening()\n\n      const body = {\n        tigger_type: type,\n        api_key: api_key,\n        text: text,\n        self_image: imageSrc,\n        face_data: faceData.current,\n        refresh_ask: refresh_ask,\n        client_user: client_user,\n      };\n      console.log(\"api\");\n      const { data } = await axios.post(api, body);\n      console.log(\"data :>> \", data, body);\n      data[\"self_image\"] && setImageSrc(data[\"self_image\"]);\n      if (audioRef.current) {\n        audioRef.current.pause(); // Pause existing playback if any\n      }\n\n      // audioRef.current = new Audio(data[\"audio_path\"]);\n      const apiUrlWithFileName = `${api_audio}${data[\"audio_path\"]}`;\n      audioRef.current = new Audio(apiUrlWithFileName);\n      audioRef.current.play();\n\n      // Wait for the onended callback to complete before continuing\n      await new Promise((resolve) => {\n        audioRef.current.onended = () => {\n          console.log(\"Audio playback finished.\");\n          resolve();\n        };\n      });\n\n      console.log(\"Audio ENDED MOVE TO SET VARS .\");\n      setAnswers(data[\"text\"]);\n      g_anwers = [...data[\"text\"]];\n      \n      setListenAfterReply(data[\"listen_after_reply\"]);\n      console.log(\"listen after reply\", data[\"listen_after_reply\"]);\n\n      if (data[\"page_direct\"] !== false && data[\"page_direct\"] !== null) {\n        console.log(\"api has page direct\", data[\"page_direct\"]);\n        // window.location.reload();\n        window.location.href = data[\"page_direct\"];\n      }\n      \n      listenContinuously();\n      setApiInProgress(false); // Set API in progress to false after completion\n\n    } catch (error) {\n      console.log(\"api call on listen failed!\", error);\n      setApiInProgress(false); // Set API in progress to false on error\n    }\n  };\n\n  useEffect(() => {\n    Streamlit.setFrameHeight();\n\n    // Check listening status every minute\n    const intervalId = setInterval(() => {\n      if (!SpeechRecognition.browserSupportsContinuousListening()) {\n        // If continuous listening is not active, start it\n        console.log(\"LISTEN STOPPED TURNING BACK ON\", error);\n        listenContinuously();\n      }\n    }, 60000);\n\n    return () => {\n      clearInterval(intervalId);\n    };\n  }, []);\n\n  const startContinuousListening = () => {\n    // Start continuous listening\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"en-GB\",\n    });\n    setIsListening(true);\n  };\n\n  const stopListening = () => {\n    SpeechRecognition.stopListening();\n  };\n  const startListening = () => {\n    SpeechRecognition.startListening();\n  };\n\n  const listenContinuously = () =>\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"en-GB\",\n    });\n  const listenContinuouslyInChinese = () =>\n    SpeechRecognition.startListening({\n      continuous: true,\n      language: \"zh-CN\",\n    });\n  const listenOnce = () =>\n    SpeechRecognition.startListening({ continuous: false });\n\n  \n  useEffect(() => {\n    const loadModels = async () => {\n      const MODEL_URL = process.env.PUBLIC_URL + \"/models\";\n\n      Promise.all([\n        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),\n        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),\n        faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),\n      ]).then(() => setModelsLoaded(true));\n    };\n    loadModels();\n    const interval = setInterval(() => {\n      console.log(\"faceData.current :>> \", faceData.current);\n    }, 3000);\n    return () => clearInterval(interval);\n  }, []);\n\n  return (\n    <>\n      <div className=\"p-2\">\n      <div>\n          {imageSrc && imageSrc.toLowerCase().endsWith(\".mp4\") ? (\n            <video\n              height={height || 100}\n              width={width || 100}\n              controls\n              autoPlay={true} // Use a variable to control autoplay shouldAutoplay\n              loop={false}\n              muted\n            >\n              <source src={imageSrc} type=\"video/mp4\" />\n              Your browser does not support the video tag.\n            </video>\n          ) : (\n            <img src={imageSrc} height={height || 100} width={width || 100} />\n          )}\n          {/* Flashing green line indicator */}\n          {apiInProgress && (\n            <div\n              style={{\n                position: 'absolute',\n                top: '10px',\n                left: '0',\n                width: '100%',\n                height: '4px',\n                background: 'linear-gradient(90deg, green, transparent 50%, green)',\n                animation: 'flashLine 1s infinite',\n              }}\n            />\n          )}\n        </div>\n        <div className=\"p-2\">\n          <Dictaphone\n            commands={commands}\n            myFunc={myFunc}\n            listenAfterReply={listenAfterReply}\n            noResponseTime={no_response_time}\n            show_conversation={show_conversation}\n            apiInProgress={apiInProgress} // Pass down API in progress\n          />\n        </div>\n        <div className=\"form-group\">\n          <button className=\"btn btn-primary\" onClick={listenContinuously}>\n            Listen continuously\n          </button>\n        </div>\n        {input_text && (\n          <div className=\"form-group\">\n            <input\n              className=\"form-control\"\n              type=\"text\"\n              placeholder=\"Chat with Hoots\"\n              value={textString}\n              onChange={handleInputText}\n              onKeyDown={handleOnKeyDown}\n            />\n          </div>\n        )}\n        {show_conversation === true && (\n          <>\n            <div> You: {message}</div>\n            {answers.map((answer, idx) => (\n              <div key={idx}>\n                <div>-user: {answer.user}</div>\n                <div>\n                  -resp: {answer.resp ? answer.resp : \"thinking...\"}\n                </div>\n              </div>\n            ))}\n          </>\n        )}\n      </div>\n      <div>\n        {/* ... (rest of your code) */}\n      </div>\n      <div>\n        {face_recon && (\n          <div style={{ textAlign: \"center\", padding: \"10px\" }}>\n            {captureVideo && modelsLoaded ? (\n              <button\n                onClick={closeWebcam}\n                style={{\n                  cursor: \"pointer\",\n                  backgroundColor: \"green\",\n                  color: \"white\",\n                  padding: \"15px\",\n                  fontSize: \"25px\",\n                  border: \"none\",\n                  borderRadius: \"10px\",\n                }}\n              >\n                Close Webcam\n              </button>\n            ) : (\n              <button\n                onClick={startVideo}\n                style={{\n                  cursor: \"pointer\",\n                  backgroundColor: \"green\",\n                  color: \"white\",\n                  padding: \"15px\",\n                  fontSize: \"25px\",\n                  border: \"none\",\n                  borderRadius: \"10px\",\n                }}\n              >\n                Open Webcam\n              </button>\n            )}\n          </div>\n        )}\n        {captureVideo ? (\n          modelsLoaded ? (\n            <div>\n              <div\n                style={{\n                  display: \"flex\",\n                  justifyContent: \"center\",\n                  padding: \"10px\",\n                  position: show_video ? \"\" : \"absolute\",\n                  opacity: show_video ? 1 : 0.3,\n                }}\n              >\n                <video\n                  ref={videoRef}\n                  height={videoHeight}\n                  width={videoWidth}\n                  onPlay={handleVideoOnPlay}\n                  style={{ borderRadius: \"10px\" }}\n                />\n                <canvas ref={canvasRef} style={{ position: \"absolute\" }} />\n              </div>\n            </div>\n          ) : (\n            <div>loading...</div>\n          )\n        ) : (\n          <></>\n        )}\n      </div>\n    </>\n  );\n};\n\nexport default CustomVoiceGPT;\n","import React, { useEffect, useState } from \"react\"\nimport {\n  ComponentProps,\n  Streamlit,\n  withStreamlitConnection,\n} from \"streamlit-component-lib\"\nimport VoiceGPT from \"./VoiceGPT.jsx\"\n\nconst Main = (props: ComponentProps) => {\n  const { api, kwargs } = props.args\n  useEffect(() => Streamlit.setFrameHeight())\n  return (\n    <>\n      <VoiceGPT api={api} kwargs={kwargs} />\n    </>\n  )\n}\n\nexport default withStreamlitConnection(Main)\n","import React from \"react\"\nimport ReactDOM from \"react-dom\"\nimport Main from \"./Main\"\n// Lots of import to define a Styletron engine and load the light theme of baseui\nimport { Client as Styletron } from \"styletron-engine-atomic\"\nimport { Provider as StyletronProvider } from \"styletron-react\"\nimport { ThemeProvider, LightTheme } from \"baseui\"\n\nconst engine = new Styletron()\n\n// Wrap your CustomSlider with the baseui them\nReactDOM.render(\n  <React.StrictMode>\n    <StyletronProvider value={engine}>\n      <ThemeProvider theme={LightTheme}>\n        <Main />\n      </ThemeProvider>\n    </StyletronProvider>\n  </React.StrictMode>,\n  document.getElementById(\"root\")\n)\n"],"sourceRoot":""}