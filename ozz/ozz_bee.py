# -*- coding: utf-8 -*-
"""Copy of Children's app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b0f7QGmPt_tK17wFOYpPkqiXHsERpq1i
"""

# !pip install python-docx
# !pip install openai
# !pip install transformers
# pip install -U openai-whisper

import pandas as pd
import numpy as np
import re
import pickle
from io import StringIO

import docx
import os
import time

import openai
from transformers import GPT2TokenizerFast
import ipdb
import argparse
from dotenv import load_dotenv
from utils.main_utils import ozz_master_root, ReadPickleData

load_dotenv(ozz_master_root())

openai.api_key = os.environ.get("ozz_api_key")

# score text generation
# However, you can use metrics like BLEU, METEOR, ROUGE, CIDEr etc to evaluate the quality of generated text.
# They are widely used to evaluate the quality of machine-generated text against the reference text.
# You can use these metrics to compare the generated text with the reference text and get a score, but keep in mind that these metrics are not perfect, and the scores they provide are not always reliable indicators of text quality.



try:
    def send_ozz_call(query):
        csv_main = 'ozz/Learning walks data.txt'
        vector_pickle = 'ozz/Learning walks embeddings.pickle'

        def doc_to_string_main():
            def docx_to_string(filename):
                try:
                    doc = docx.Document(filename)  # Creating word reader object.
                    data = ""
                    fullText = []
                    for para in doc.paragraphs:
                        fullText.append(para.text)
                        data = '\n'.join(fullText)
            
                    return data
            
                except IOError:
                    print('There was an error opening the file!')

            filename = 'Learning walks_Main.docx'

            contents = docx_to_string(filename)

            df = pd.DataFrame([[filename, contents]], columns = ["File name", "contents"])

        MAX_SECTION_LEN = 3000
        SEPARATOR = "\n* "

        def preprocessing(df): 
            #Removing unwanted characters
            for i in range(df.shape[0]):
                df['contents'][i] = df['contents'][i].replace('\n', '')
                df['contents'][i] = re.sub(r'\(.*?\)', '', df['contents'][i])
                df['contents'][i] = re.sub('[\(\[].*?[\)\]]', '', df['contents'][i])

            prompt_column = []
            completion_column = []

            num_parts = (-df['contents'].map(len).max()//-1000)

            for i in df['File name']:
                for part_num in range(num_parts):
                    prompt_column.append(i.lower() + " part" + str(part_num+1))



            for j in df['contents']:
                
                split_data = j.split('.')
                avg_len = len(split_data)//num_parts + 1
                for part_num in range(num_parts - 1):
                    completion_column.append('.'.join(split_data[part_num*avg_len:(part_num+1)*avg_len]))

                completion_column.append('.'.join(split_data[(num_parts - 1)*avg_len:]))

            df_cleaned = pd.DataFrame()
            df_cleaned['File name'] = prompt_column
            df_cleaned['contents'] = completion_column

                    
            return df_cleaned[df_cleaned['contents'] != '']


        def count_tokens(input):
            tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
            res = tokenizer(input)['input_ids']

            return len(res)
        # df_cleaned['tokens'] = df_cleaned['contents'].map(count_tokens)
        # df_cleaned.to_csv('Learning walks data.csv', index = False)

        df_cleaned = pd.read_csv(csv_main)


        context_embeddings = ReadPickleData(vector_pickle)

        embedding_model = "text-embedding-ada-002"
        MODEL_NAME = "davinci"

        # DOC_EMBEDDINGS_MODEL = f"text-search-{MODEL_NAME}-doc-001"
        # QUERY_EMBEDDINGS_MODEL = f"text-search-{MODEL_NAME}-query-001"

        def get_embedding(text, model=embedding_model):
            text = text.replace("\n", " ")
            return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']

        def get_doc_embedding(text):
            return get_embedding(text)

        def get_query_embedding(text):
            return get_embedding(text)

        def compute_doc_embeddings(df):

            return {
                idx: get_doc_embedding(r.contents.replace("\n", " ")) for idx, r in df.iterrows()
            }

        def train_model(pickle_file):
            ## Train Model

            context_embeddings = compute_doc_embeddings(df_cleaned)

            with open(pickle_file, 'wb') as pkl:
                pickle.dump(context_embeddings, pkl)

        def vector_similarity(x, y):

            return np.dot(np.array(x), np.array(y))

        def order_document_sections_by_query_similarity(query, contexts):

            query_embedding = get_query_embedding(query)
            
            document_similarities = sorted([
                (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()
            ], reverse=True)
            
            return document_similarities

        def construct_prompt(question, context_embeddings, df):

            most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)
            
            chosen_sections = []
            chosen_sections_len = 0
            chosen_sections_indexes = []
            
            for _, section_index in most_relevant_document_sections:
                # Add contexts until we run out of space.        
                document_section = df.loc[section_index]
                
                chosen_sections_len += document_section.tokens + 3
                if chosen_sections_len > MAX_SECTION_LEN:
                    break
                    
                chosen_sections.append(SEPARATOR + document_section.contents.replace("\n", " "))
                chosen_sections_indexes.append(str(section_index))
                    
            # Useful diagnostic information
            print(f"Selected {len(chosen_sections)} document sections:")
            print("\n".join(chosen_sections_indexes))
            
            header = """Answer the question as truthfully as possible only using the provided context. and if the answer is not contained within the text below, say "I don't know" \n\nContext:\n"""
            
            return header + "".join(chosen_sections) + "\n\n Q: " + question + "\n A:"

        COMPLETIONS_MODEL = "text-davinci-003"

        COMPLETIONS_API_PARAMS = {
            "temperature": 0.0,
            "max_tokens": 300,
            "model": COMPLETIONS_MODEL,
        }

        def answer_query_with_context(query, df, context_embeddings,show_prompt= False):
            prompt = construct_prompt(
                query,
                context_embeddings,
                df
            )
            
            if show_prompt:
                print(prompt)

            response = openai.Completion.create(
                        prompt=prompt,
                        **COMPLETIONS_API_PARAMS
                    )

            return response["choices"][0]["text"].strip(" \n")

        # command = 'Y'

        resp = answer_query_with_context(query, df=df_cleaned, context_embeddings=context_embeddings)
        # st.write(resp)
        
        return resp
    # def send_ozz_call(query):
        # return answer_query_with_context(query, df=df_cleaned, context_embeddings=context_embeddings)

    # while command == 'Y':

    #   query = input("\nAsk me anything...\n")

    #   print('\n', answer_query_with_context(query, df=df_cleaned, context_embeddings=context_embeddings))

    #   command = input("\n\nWould you like to continue? Y or N : ")

except Exception as e:
    print(e)
    # print_line_of_error()

if __name__ == '__main__':
    def createParser():
        parser = argparse.ArgumentParser()
        parser.add_argument ('-query', default="Tell me About the Clouds")
        return parser
    parser = createParser()
    namespace = parser.parse_args()
    query = namespace.query
    send_ozz_call(query)